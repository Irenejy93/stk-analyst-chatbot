{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07984c3e-df7c-4d64-b0d3-c1c5ce07a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/stk-analyst-chatbot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd  /workspace/stk-analyst-chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6aabc4-f461-439d-b695-98e249f81e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(f'./data/final_multiturns_wnofun.json' , 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5239f2bd-90e1-499f-acfa-adacae5329de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.0\n",
      "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.2.0)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch==2.2.0)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (23.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
      "Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m207.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m175.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m143.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m216.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m159.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, typing-extensions, triton, tensorboard-data-server, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markdown, grpcio, absl-py, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.2.1 grpcio-1.71.0 markdown-3.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.1.105 protobuf-6.30.2 tensorboard-2.19.0 tensorboard-data-server-0.7.2 torch-2.2.0 triton-2.2.0 typing-extensions-4.13.0 werkzeug-3.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==3.0.1\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate==0.4.3\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting bitsandbytes==0.44.0\n",
      "  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting trl==0.11.1\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting qwen-vl-utils\n",
      "  Downloading qwen_vl_utils-0.0.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets==3.0.1)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.0.1)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers==4.45.1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets==3.0.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2023.4.0)\n",
      "Collecting aiohttp (from datasets==3.0.1)\n",
      "  Downloading aiohttp-3.11.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.2.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.1)\n",
      "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting av (from qwen-vl-utils)\n",
      "  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (9.3.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading multidict-6.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets==3.0.1)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.8.93)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m172.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading qwen_vl_utils-0.0.10-py3-none-any.whl (6.7 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m208.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.2/481.2 kB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m185.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m152.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m153.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m195.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.18-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m197.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (230 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.7/230.7 kB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typeguard, tqdm, threadpoolctl, shtab, scipy, safetensors, requests, regex, pyarrow, propcache, multidict, mdurl, joblib, fsspec, frozenlist, docstring-parser, dill, av, async-timeout, aiohappyeyeballs, yarl, scikit-learn, qwen-vl-utils, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, bitsandbytes, accelerate, peft, datasets, trl, evaluate\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.11.15 aiosignal-1.3.2 async-timeout-5.0.1 av-14.2.0 bitsandbytes-0.44.0 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 evaluate-0.4.3 frozenlist-1.5.0 fsspec-2024.6.1 huggingface-hub-0.30.1 joblib-1.4.2 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.3.1 multiprocess-0.70.16 pandas-2.2.3 peft-0.13.0 propcache-0.3.1 pyarrow-19.0.1 pytz-2025.2 qwen-vl-utils-0.0.10 regex-2024.11.6 requests-2.32.3 rich-14.0.0 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 shtab-1.7.1 threadpoolctl-3.6.0 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.45.1 trl-0.11.1 typeguard-4.4.2 tyro-0.9.18 tzdata-2025.2 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch==2.2.0\" tensorboard pillow\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\" \\\n",
    "  \"scikit-learn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a385b3d9-feee-4a77-97cf-e1be9ea0a9b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ninja\n",
      "Successfully installed ninja-1.11.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.2.0)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.13.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187696268 sha256=1776769f7ae3a8be3b31ec3a4c875ad1764da74be2d9b1751e5c01162ad0096f\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.1 flash-attn-2.7.4.post1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# GPU가 Flash Attention을 지원하는지 확인\n",
    "assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "\n",
    "# Flash Attention 설치\n",
    "!pip install ninja packaging\n",
    "!pip install flash-attn --no-build-isolation --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d57dcc-2f92-4cca-965a-c92f9208c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\",\n",
    "       \"function\":   {\n",
    "        \"name\":'get_earnings',\n",
    "        \"description\":'당신은 기업의 실적, 재무재표 또는 현금흐름을 가져오고 분석하는 로봇입니다.',\n",
    "        'parameters':{\n",
    "            \"type\":\"object\",\n",
    "            \"properties\" :{\n",
    "                \"symbol\":{'type':'string', \"description\": '실적 데이터를 찾고자 하는 기업의 symbol'},\n",
    "                'analysis_type':{'type':'string', \"description\":\"\"\"기업의 재무 데이터에서 어떤 정보를 추출하고자 하는지 출력하는 파라미터입니다.\n",
    "                매출 성장률, 순이익 성장률, 영업이익 성장률 과 같은 기업의 성장성을 요청할경우 'growth'라고 출력,\n",
    "                얼마나 효율적으로 이익을 창출하는지 수익성과 관련한 질문일경우 'profitability' 출력,\n",
    "                부채 수준과 재무 건전성과 관련한 질문일경우 'stability' 출력,\n",
    "                현재 가치와 적정 주가와 관련한 질문일경우 'valuation' 출력,\n",
    "                현금을 얼마나 잘 창출하는지 확인하여 재무건전성과 관련한 질문일경우 'cashflow' 출력,\n",
    "                배당 정책과 주식 발행 내역과 관련한 질문일경우 'dividend' 출력,\n",
    "                비용 절감 능력과 효율성과 관련한 질문일경우 'cost' 출력,\n",
    "                그외의 재무,실적,현금흐름과 연관이있지만 파라미터를 찾을수없는경우 'NA'라고 출력합니다.\n",
    "\n",
    "                \"\"\"},\n",
    "                \"type_\":{'type':'string', \"description\": \"연간 데이터 일경우 'yearly' 아닐경우 'quarter' 이라고 표기\"},\n",
    "                \"year\":{'type':'string', \"description\": '데이터를 찾고자 하는 연도, 명시 하지 않을경우 가장 최근 연도 로 설정, 1월일경우 , 해당연도에 데이터가 입력되지않을수있음 따라서 최근연도-1 값을 연도라고 표기 '},\n",
    "                \"quarter\":{'type':'string', \"description\": '데이터를 찾고자 하는 분기로 1~4로 이루어진 숫자, 마지막 또는 최근 분기일경우 또는 언급이 없을경우 -1 이라고 표기 '}\n",
    "            },\n",
    "            \"required\":['symbol', 'analysis_type','type_', 'year','quarter'],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "             \"strict\": True\n",
    "    }},\n",
    "         {\"type\": \"function\",\n",
    "       \"function\":   {\n",
    "        \"name\":'get_consensus',\n",
    "        \"description\":'당신은 기업의 (EPS) 컨센서스 데이터 또는, 매수/매도/홀드 의견 목표주가를 가져와서 분석하는 로봇입니다.',\n",
    "        'parameters':{\n",
    "            \"type\":\"object\",\n",
    "            \"properties\" :{\n",
    "                \"symbol\":{'type':'string', \"description\": ' 데이터를 찾고자 하는 기업의 symbol'},\n",
    "                \"year\":{'type':'string', \"description\": '데이터를 찾고자 하는 연도, 명시 하지 않을경우 가장 최근 연도 로 설정, 1월일경우 , 해당연도에 데이터가 입력되지않을수있음 따라서 최근연도-1 값을 연도라고 표기 '},\n",
    "                \"quarter\":{'type':'string', \"description\": '데이터를 찾고자 하는 분기로 1~4로 이루어진 숫자, 최근 또는 마지막일경우 -1 이라고 표기 '}\n",
    "            },\n",
    "            \"required\":['symbol', 'year','quarter'],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "             \"strict\": True\n",
    "    }},\n",
    "        {\"type\": \"function\",\n",
    "         \"function\":    {\n",
    "        \"name\":'get_issues_summarized',\n",
    "        \"description\":'당신은 특정 기업 또는 키워드의 이슈검색 챗봇입니다.',\n",
    "        'parameters':{\n",
    "            \"type\":\"object\",\n",
    "            \"properties\" :{\n",
    "                \"keyword\":{'type':'string', \"description\": '이슈/현황을 검색하고자 하는 회사명 또는 키워드 in English '},\n",
    "                \"days\":{'type':'integer', \"description\": '검색하고자 하는 기간 예시: 하루, 일주일, 한달, 등등'}\n",
    "\n",
    "            },\n",
    "            \"required\":['keyword','days'],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "             \"strict\": True\n",
    "        }\n",
    "    },\n",
    "    {\"type\": \"function\",\n",
    "       \"function\":   {\n",
    "        \"name\":'get_reddit_hotissue',\n",
    "        \"description\":'당신은 (금융)시장에서 핫한 이슈를 요약하는 챗봇입니다.',\n",
    "        'parameters':{\n",
    "            \"type\":\"object\",\n",
    "            \"properties\" :{\n",
    "                \"days\":{'type':'integer', \"description\": '검색하고자 하는 기간 예시: 하루, 일주일, 한달,일년 등 **질문에 언급이 없을경우 일주일로 지정'}\n",
    "\n",
    "            },\n",
    "            \"required\":['days'],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "             \"strict\": True\n",
    "    }}]\n",
    "\n",
    "system_prompt = f\"\"\"당신은 다양한 기능을 호출할 수 있는 AI 모델입니다. 사용자의 요청에 따라 특정 함수를 호출하고, 함수의 시그니처는 <tools></tools> XML 태그 내에 제공됩니다. 함수를 호출할 때는 함수에 필요한 정확한 값을 추정하지 말고 사용자가 제공한 값에 따라 함수를 실행해야 합니다.\n",
    "아래는 사용 가능한 함수들과 각각의 파라미터에 대한 설명입니다:{str(tools)}\"\"\" + '''** 함수 호출 예시는 다음과 같습니다:\n",
    "<tool_call>\n",
    "{\n",
    "    \"name\": \"get_earnings\",\n",
    "    \"arguments\": {\n",
    "        \"symbol\": \"AAPL\",\n",
    "        \"analysis_type\": \"growth\",\n",
    "        \"type_\": \"yearly\" ,\n",
    "        \"year\": \"2024\",\n",
    "        \"quarter\": \"-1\"\n",
    "    }\n",
    "}\n",
    "</tool_call>\n",
    "**주의사항\n",
    "- 각 함수의 인자 값을 정확하게 지정해 주세요. 특히, 연도와 분기를 설정할 때 현재 날짜가 1월이나 2월인 경우, 최근 연도의 데이터를 참조하도록 주의해야 합니다.\n",
    "- 함수를 호출할때를 제외하고 한국어로 대답하세요.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f091aa6-e7bc-4e15-82f2-fb7d2d85c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tools = str(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a423c46-9261-40cb-801d-53bca5c3fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messeges = [{'message':d} for d in data]\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset , test_dataset = train_test_split(messeges, test_size=0.1, random_state=42)\n",
    "dataset = Dataset.from_list(train_dataset)\n",
    "import json\n",
    "# with open('data/train.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_dataset)\n",
    "with open('data/test.json', 'w') as f:\n",
    "    json.dump(test_dataset,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e224a3b-9f73-44a5-8755-dc97c089bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_data(example):\n",
    "    # 날짜 추출\n",
    "    date = example[0]['content'].split('\\n')[-1].strip()\n",
    "    prompt0 = system_prompt + f\"오늘 날짜는 {date} 입니다.\"\n",
    "    system_message = {\n",
    "        \"content\": prompt0,\n",
    "        \"role\": \"system\"\n",
    "    }\n",
    "\n",
    "    # 메시지 리스트를 복사해서 가공\n",
    "    messages = [system_message] + example[1:]\n",
    "\n",
    "    # tool_response 처리\n",
    "    for i, msg in enumerate(messages):\n",
    "        if msg['role'] == 'user' and 'tool_response' in msg['content']:\n",
    "            # 자르고 닫는 태그 덧붙이기\n",
    "            messages[i] = {\n",
    "                'content': msg['content'][:4000] + '</tool_response>',\n",
    "                'role': 'user'\n",
    "            }\n",
    "\n",
    "    return {'messages': messages}\n",
    "\n",
    "# 예시: data는 여러 개의 대화 세트로 이루어진 리스트\n",
    "# 각 대화 세트는 [{'role': ..., 'content': ...}, {...}, ...] 형식의 리스트\n",
    "formatted_data = [format_data(sample) for sample in data]\n",
    "\n",
    "# Hugging Face Dataset 객체로 변환\n",
    "dataset = Dataset.from_list(formatted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480cfe44-918e-4df4-8022-055efe24e78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c521067ce8c41799f71e7b20f315fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c83e4ef93249bf9c1cc59db25da8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e839fb29d86743f786ed27c1e206c288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d9d0eeda754095a614e60eb27c6c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a596dcd95623462fb5f25dc453ecee07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc207386c371401a81433acb869aea73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7570938cd99492f97978dedab886ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50112e200a4b45e3a2c003371054b1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1724356de3c0432c9a393d514242c690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dab7e5d4aa441bb2fe2a1928550cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd0c915f67a481fbce12e4b7b0f2358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c413bb5078744ad79508c0953ed45e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef3e91d031d4d398619d4c335711d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\" \n",
    "\n",
    "# BitsAndBytes 4비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,                             # 4비트 양자화 사용\n",
    "   bnb_4bit_use_double_quant=True,               # 이중 양자화 사용으로 메모리 추가 절약\n",
    "   bnb_4bit_quant_type=\"nf4\",                    # 4비트 양자화 타입 설정(normalized float 4)\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16         # 연산 시 bfloat16 타입 사용\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af2680af-7446-4e99-a258-d952ac34458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 다양한 기능을 호출할 수 있는 AI 모델입니다. 사용자의 요청에 따라 특정 함수를 호출하고, 함수의 시그니처는 <tools></tools> XML 태그 내에 제공됩니다. 함수를 호출할 때는 함수에 필요한 정확한 값을 추정하지 말고 사용자가 제공한 값에 따라 함수를 실행해야 합니다.\n",
      "아래는 사용 가능한 함수들과 각각의 파라미터에 대한 설명입니다:[{'type': 'function', 'function': {'name': 'get_earnings', 'description': '당신은 기업의 실적, 재무재표 또는 현금흐름을 가져오고 분석하는 로봇입니다.', 'parameters': {'type': 'object', 'properties': {'symbol': {'type': 'string', 'description': '실적 데이터를 찾고자 하는 기업의 symbol'}, 'analysis_type': {'type': 'string', 'description': \"기업의 재무 데이터에서 어떤 정보를 추출하고자 하는지 출력하는 파라미터입니다.\\n                매출 성장률, 순이익 성장률, 영업이익 성장률 과 같은 기업의 성장성을 요청할경우 'growth'라고 출력,\\n                얼마나 효율적으로 이익을 창출하는지 수익성과 관련한 질문일경우 'profitability' 출력,\\n                부채 수준과 재무 건전성과 관련한 질문일경우 'stability' 출력,\\n                현재 가치와 적정 주가와 관련한 질문일경우 'valuation' 출력,\\n                현금을 얼마나 잘 창출하는지 확인하여 재무건전성과 관련한 질문일경우 'cashflow' 출력,\\n                배당 정책과 주식 발행 내역과 관련한 질문일경우 'dividend' 출력,\\n                비용 절감 능력과 효율성과 관련한 질문일경우 'cost' 출력,\\n                그외의 재무,실적,현금흐름과 연관이있지만 파라미터를 찾을수없는경우 'NA'라고 출력합니다.\\n\\n                \"}, 'type_': {'type': 'string', 'description': \"연간 데이터 일경우 'yearly' 아닐경우 'quarter' 이라고 표기\"}, 'year': {'type': 'string', 'description': '데이터를 찾고자 하는 연도, 명시 하지 않을경우 가장 최근 연도 로 설정, 1월일경우 , 해당연도에 데이터가 입력되지않을수있음 따라서 최근연도-1 값을 연도라고 표기 '}, 'quarter': {'type': 'string', 'description': '데이터를 찾고자 하는 분기로 1~4로 이루어진 숫자, 마지막 또는 최근 분기일경우 또는 언급이 없을경우 -1 이라고 표기 '}}, 'required': ['symbol', 'analysis_type', 'type_', 'year', 'quarter'], 'additionalProperties': False}, 'strict': True}}, {'type': 'function', 'function': {'name': 'get_consensus', 'description': '당신은 기업의 (EPS) 컨센서스 데이터 또는, 매수/매도/홀드 의견 목표주가를 가져와서 분석하는 로봇입니다.', 'parameters': {'type': 'object', 'properties': {'symbol': {'type': 'string', 'description': ' 데이터를 찾고자 하는 기업의 symbol'}, 'year': {'type': 'string', 'description': '데이터를 찾고자 하는 연도, 명시 하지 않을경우 가장 최근 연도 로 설정, 1월일경우 , 해당연도에 데이터가 입력되지않을수있음 따라서 최근연도-1 값을 연도라고 표기 '}, 'quarter': {'type': 'string', 'description': '데이터를 찾고자 하는 분기로 1~4로 이루어진 숫자, 최근 또는 마지막일경우 -1 이라고 표기 '}}, 'required': ['symbol', 'year', 'quarter'], 'additionalProperties': False}, 'strict': True}}, {'type': 'function', 'function': {'name': 'get_issues_summarized', 'description': '당신은 특정 기업 또는 키워드의 이슈검색 챗봇입니다.', 'parameters': {'type': 'object', 'properties': {'keyword': {'type': 'string', 'description': '이슈/현황을 검색하고자 하는 회사명 또는 키워드 in English '}, 'days': {'type': 'integer', 'description': '검색하고자 하는 기간 예시: 하루, 일주일, 한달, 등등'}}, 'required': ['keyword', 'days'], 'additionalProperties': False}, 'strict': True}}, {'type': 'function', 'function': {'name': 'get_reddit_hotissue', 'description': '당신은 (금융)시장에서 핫한 이슈를 요약하는 챗봇입니다.', 'parameters': {'type': 'object', 'properties': {'days': {'type': 'integer', 'description': '검색하고자 하는 기간 예시: 하루, 일주일, 한달,일년 등 **질문에 언급이 없을경우 일주일로 지정'}}, 'required': ['days'], 'additionalProperties': False}, 'strict': True}}]** 함수 호출 예시는 다음과 같습니다:\n",
      "<tool_call>\n",
      "{\n",
      "    \"name\": \"get_earnings\",\n",
      "    \"arguments\": {\n",
      "        \"symbol\": \"AAPL\",\n",
      "        \"analysis_type\": \"growth\",\n",
      "        \"type_\": \"yearly\" ,\n",
      "        \"year\": \"2024\",\n",
      "        \"quarter\": \"-1\"\n",
      "    }\n",
      "}\n",
      "</tool_call>\n",
      "**주의사항\n",
      "- 각 함수의 인자 값을 정확하게 지정해 주세요. 특히, 연도와 분기를 설정할 때 현재 날짜가 1월이나 2월인 경우, 최근 연도의 데이터를 참조하도록 주의해야 합니다.\n",
      "- 함수를 호출할때를 제외하고 한국어로 대답하세요.오늘 날짜는 오늘 날짜는 2022년 04월 03일 입니다. 입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "금융시장에서 최근 일주일간의 주요 이슈는 무엇인가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<tool_call>{\"name\":\"get_reddit_hotissue\",\"arguments\": {\"days\": 7}}</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "<tool_reponse>title :What Are Your Moves Tomorrow, March 20, 2025\n",
      "body :This post contains content not supported on old Reddit. [Click here to view the full post](https://sh.reddit.com/r/wallstreetbets/comments/1jf65ux)\n",
      "\n",
      "title :Tesla (TSLA) accounting raises red flags as report shows $1.4 billion missing\n",
      "body :\n",
      "\n",
      "title :Every single FED’s conference should look like this\n",
      "body :\n",
      "\n",
      "title :Trump still intends for reciprocal tariffs to kick in on April 2, White House says\n",
      "body :Summary\n",
      "\n",
      "* White House official says tariffs to take effect April 2\n",
      "* Negotiations to lower tariffs needed ahead of April 2Countries to get tariff number on April 2, Bessent says\n",
      "* Bessent sees opportunity to negotiate tariffs lower\n",
      "* USTR wrestling with design of complex reciprocal tariff plan\n",
      "\n",
      "WASHINGTON, March 18 (Reuters) - U.S. President [Donald Trump](https://www.reuters.com/world/us/donald-trump/) still intends for new reciprocal tariff rates to take effect on April 2, the White House said on Tuesday, despite earlier comments from Treasury Secretary Scott Bessent that indicated a possible delay in their activation.\"The intent is to enact tariffs on April 2,\" the official said when asked to clarify Bessent's comments that countries would get an opportunity to avoid higher tariffs by reducing their own trade barriers.\n",
      "\n",
      "Get weekly news and analysis on U.S. politics and how it matters to the world with the Reuters Politics U.S. newsletter. Sign up [here.](https://www.reuters.com/newsletters/reuters-politics-us/?location=article-paragraph)\n",
      "\n",
      "\"Unless the tariff and non-tariff barriers are equalized, or the U.S. has higher tariffs, the tariffs will go into effect,\" the White House official said.Bessent told Fox Business Network's \"Mornings with Maria\" program that Trump on April 2 would give trading partner countries a [reciprocal tariff](https://www.reuters.com/world/us/trump-says-reciprocal-tariffs-coming-thursday-2025-02-13/) number that reflects their own rates, non-tariff trade barriers, currency practice and other factors, but could negotiate to avoid a \"tariff wall.\"\"On April 2, each country will receive a number that we believe represents their tariffs,\" Bessent said. \"For some countries, it could be quite low, for some countries, it could be quite high.\"\"We are going to go to them and say, 'Look, here's where we think the tariff levels are, non-tariff barriers, currency manipulation, unfair funding, labor suppression, and if you will stop this, we will not put up the tariff wall,'\" Bessent said</tool_response></tool_response><|im_end|>\n",
      "<|im_start|>assistant\n",
      "최근 일주일간 금융시장에서 주목받고 있는 주요 이슈는 다음과 같습니다:\n",
      "\n",
      "1. **Tesla (TSLA)의 회계 문제**: Tesla의 회계에서 1.4억 달러가 사라졌다는 보고서가 나오면서 경고 신호가 제기되었습니다.\n",
      "\n",
      "2. **트럼프 대통령의 상호 관세 정책**: 백악관은 4월 2일에 새로운 상호 관세율이 시행될 것이라고 밝혔습니다. 이에 따라 각국은 그들의 관세 및 비관세 장벽을 낮추기 위한 협상이 필요할 것으로 보입니다. 미국 통상대표부(USTR)는 복잡한 상호 관세 계획의 디자인을 두고 씨름 중입니다. \n",
      "\n",
      "다른 이슈로는 연준의 회의에 대한 관심과 구체적인 내용은 제공되지 않았습니다.\n",
      "\n",
      "이슈들은 금융 및 정치적 요소들로 구성되어 있으며, 특히 Tesla의 회계 문제와 트럼프 대통령의 상호 관세 발표가 주목받고 있습니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "최근에 언급된 Tesla의 회계 문제에 대한 반응은 어떠한가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Tesla의 회계 문제와 관련하여 추가적인 언급이나 반응에 대한 정보는 제공된 내용 안에는 포함되어 있지 않습니다. 그러나 일반적으로 이런 회계 문제는 투자자들 사이에서 우려를 불러일으킬 수 있으며, Tesla의 주가나 시장 내 평판에 영향을 미칠 가능성이 큽니다. 이러한 이슈에 대한 더 구체적인 반응은 보통 뉴스 기사나 금융 분석 보고서를 통해 확인할 수 있습니다. 추가적인 정보를 원하시면 니즈에 따라 검색을 하거나 관련된 최신 뉴스를 참고하시길 권장합니다.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7a88eb-c0af-42a5-a857-bb0cc4cf78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    " \n",
    "# QLoRA 논문 및 Sebastian Raschka 실험에 기반한 LoRA Conifg\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# from transformers import TrainingArguments\n",
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2.5-7b-multiturn_fc\",     # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=10,                      # 학습할 총 에포크 수\n",
    "    per_device_train_batch_size=4,           # 장치당 학습 배치 크기\n",
    "    gradient_accumulation_steps=4,           # 역전파/가중치 업데이트 전 누적할 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 그래디언트 체크포인팅 사용\n",
    "    optim=\"adamw_torch_fused\",               # 퓨즈드 AdamW 옵티마이저 사용\n",
    "    logging_steps=10,                        # 10스텝마다 로그 기록\n",
    "    save_strategy=\"steps\",                   # 특정 스텝마다 체크포인트 저장\n",
    "    save_steps=50,\n",
    "    bf16=True,                              # bfloat16 정밀도 사용\n",
    "    tf32=True,                              # tf32 정밀도 사용\n",
    "    learning_rate=1e-4,                     # 학습률 (QLoRA 논문 기반)\n",
    "    max_grad_norm=0.3,                      # 최대 그래디언트 노름 (QLoRA 논문 기반)\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율 (QLoRA 논문 기반)\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률 스케줄러 사용\n",
    "    push_to_hub=False,                      # 허브에 모델 업로드 안 함\n",
    "    report_to=\"tensorboard\",                # 텐서보드에 메트릭 기록\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4b993f-2e3c-4412-b04d-6cf6f838a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        # messages의 각 내용에서 개행문자 제거\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        # 깨끗해진 메시지로 템플릿 적용\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        # 텍스트를 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # 레이블 초기화\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # assistant 응답 부분 찾기\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        # 토큰 ID 가져오기\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # <|im_start|>assistant 찾기\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                # assistant 토큰 찾기\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    # assistant 응답의 시작 위치로 이동\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    # <|im_end|>를 찾을 때까지 레이블 설정\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "                            # <|im_end|> 토큰도 레이블에 포함\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    # 패딩 적용\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch\n",
    "\n",
    "def print_tokens_and_labels(batch):\n",
    "    input_ids = batch[\"input_ids\"][0].tolist()\n",
    "    labels = batch[\"labels\"][0].tolist()\n",
    "    \n",
    "    print(\"\\n토큰과 레이블 비교:\")\n",
    "    print(f\"{'Token ID':<10} {'Token':<30} {'Label':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for token_id, label in zip(input_ids, labels):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        label_str = str(label) if label != -100 else \"-100\"\n",
    "        print(f\"{token_id:<10} {token:<30} {label_str:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc830518-ccff-4b01-9436-d77d6bc07725",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70e8172e-d095-46dc-ac8b-a46b98e61393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 2458])\n",
      "어텐션 마스크 형태: torch.Size([1, 2458])\n",
      "레이블 형태: torch.Size([1, 2458])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "max_seq_length = 9216  \n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8890e5a5-c8c5-492f-98fe-bac9787733a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 151657, 4913, 606, 3252, 455, 1288, 20090, 33433, 11159, 2198, 16370, 788, 5212, 13778, 788, 220, 22, 3417, 151658, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 128215, 125722, 83556, 54330, 32077, 62275, 40771, 230, 128024, 133627, 56475, 55673, 87608, 132872, 34395, 64521, 55673, 35711, 23084, 144018, 16560, 139107, 138691, 1447, 16, 13, 3070, 89441, 320, 51, 7984, 32, 8, 20401, 98005, 124781, 126674, 95518, 27199, 20401, 98005, 124781, 56475, 220, 16, 13, 19, 127475, 34143, 105, 60294, 19969, 32129, 50340, 128036, 130822, 63332, 34395, 26698, 19969, 137298, 131611, 43115, 34395, 128753, 47324, 19969, 62071, 20487, 141167, 382, 17, 13, 3070, 28626, 125894, 126445, 136774, 20401, 58034, 47324, 92751, 41429, 36055, 126712, 95518, 22042, 109, 131893, 124780, 33704, 220, 19, 128514, 220, 17, 32077, 19391, 134585, 58034, 47324, 92751, 41429, 132841, 12802, 138767, 131396, 71108, 130939, 22042, 251, 144472, 38231, 13, 23084, 19391, 126629, 126804, 124785, 33704, 54825, 129360, 92751, 41429, 128355, 73986, 124780, 41429, 129359, 141479, 17877, 37195, 106, 132526, 20487, 130679, 47455, 239, 55902, 12802, 126871, 47836, 132091, 63332, 78952, 13, 132662, 125206, 55902, 66845, 126414, 63089, 12317, 6666, 8, 16560, 30520, 113, 132499, 23573, 58034, 47324, 92751, 41429, 94203, 127324, 20401, 76497, 242, 25715, 31328, 17877, 129419, 34395, 3315, 242, 101, 63154, 70943, 78952, 13, 4710, 13146, 82965, 23084, 144018, 17380, 16560, 77353, 129044, 20401, 98005, 20401, 19391, 128605, 138449, 53680, 58777, 49543, 128533, 130213, 33704, 130094, 132553, 133995, 38231, 382, 12802, 144018, 128901, 40771, 230, 128024, 128355, 136994, 80968, 85997, 43590, 64850, 17380, 136239, 128993, 132931, 11, 136115, 27199, 20401, 98005, 124781, 126674, 80573, 127819, 116, 125894, 126445, 136774, 20401, 58034, 47324, 92751, 41429, 142234, 19969, 55673, 87608, 132872, 34395, 128472, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 89441, 20401, 98005, 124781, 126674, 80573, 129985, 82190, 68408, 128533, 139957, 128911, 129835, 63757, 131518, 19391, 128605, 60039, 16560, 130094, 52300, 130213, 95170, 126377, 133970, 128993, 141231, 138539, 13, 130549, 134664, 128552, 129007, 98005, 124781, 126674, 16560, 10764, 230, 105, 25715, 25715, 64850, 136541, 56475, 124657, 125476, 18411, 126488, 60294, 32077, 33509, 143651, 28733, 132931, 11, 27199, 20401, 55673, 19969, 60315, 44518, 40853, 66136, 69441, 231, 129382, 19391, 126440, 129321, 17877, 125714, 142588, 95351, 137032, 45313, 121, 21953, 13, 131367, 23084, 144018, 19391, 128605, 126366, 58777, 49543, 128533, 63757, 131518, 33704, 63332, 125160, 5140, 231, 112, 24897, 54116, 55054, 60315, 40771, 230, 128024, 128618, 129150, 63332, 34395, 26698, 18411, 131582, 73859, 47836, 28733, 128472, 13, 68408, 128533, 60039, 18411, 129093, 127268, 32290, 34143, 230, 134133, 19391, 126629, 85322, 77226, 17877, 53900, 127451, 129985, 52300, 81173, 82528, 5140, 231, 112, 24897, 18411, 142616, 127268, 130105, 142452, 40853, 60838, 13, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa8673c-ff0b-4e6c-a495-d6ef174da621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 9216 # 모델과 데이터셋 패킹을 위한 최대 시퀀스 길이\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef7a0c64-dd02-4a90-9b6f-81f39b6e94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a44cd-f178-49dc-bca2-6dfbfa197f42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1862' max='2170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1862/2170 02:52 < 1:28:20, 0.06 it/s, Epoch 8.57/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.625600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train(resume_from_checkpoint=\"qwen2.5-7b-multiturn_fc/checkpoint-1850\")   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "#\"resume_from_checkpoint=\"qwen2.5-7b-functioncall/checkpoint-1200\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f2c08b-6737-49bb-84c2-2eb925dad7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-09 11:20:19--  https://raw.githubusercontent.com/ukairia777/LLM-Finetuning-tutorial/main/merge.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "Length: 1351 (1.3K) [text/plain]\n",
      "Saving to: ‘merge.py’\n",
      "\n",
      "merge.py            100%[===================>]   1.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-09 11:20:19 (69.6 MB/s) - ‘merge.py’ saved [1351/1351]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장\n",
    "!wget https://raw.githubusercontent.com/ukairia777/LLM-Finetuning-tutorial/main/merge.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28c5a8a-b143-4258-8a5a-64d24b427881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d68dd06-77e7-448f-8ef9-982c06a95000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2\n",
      "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
      "Collecting triton==2.2.0 (from torch==2.2)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2) (1.3.0)\n",
      "Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 torch-2.2.0 triton-2.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aef4ef3-edc8-42a5-94fc-8cf474aff952",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput_dir\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "142ca8d2-d627-4f05-b52e-829a8e1b761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Qwen/Qwen2.5-7B-Instruct\n",
      "config.json: 100%|█████████████████████████████| 663/663 [00:00<00:00, 8.74MB/s]\n",
      "model.safetensors.index.json: 100%|████████| 27.8k/27.8k [00:00<00:00, 88.5MB/s]\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/3.95G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|     | 41.9M/3.95G [00:00<00:11, 345MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏     | 105M/3.95G [00:00<00:09, 405MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏     | 157M/3.95G [00:00<00:08, 449MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎     | 210M/3.95G [00:00<00:08, 434MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▍     | 262M/3.95G [00:00<00:08, 459MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍     | 315M/3.95G [00:00<00:09, 386MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▌     | 357M/3.95G [00:00<00:09, 378MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▌     | 409M/3.95G [00:00<00:08, 408MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▋     | 472M/3.95G [00:01<00:07, 451MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▊     | 535M/3.95G [00:01<00:07, 483MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▉     | 587M/3.95G [00:01<00:07, 440MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▉     | 640M/3.95G [00:01<00:07, 438MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|█     | 703M/3.95G [00:01<00:06, 470MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|█▏    | 755M/3.95G [00:01<00:06, 473MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|█▏    | 807M/3.95G [00:01<00:06, 455MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|█▎    | 860M/3.95G [00:01<00:06, 467MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|█▍    | 912M/3.95G [00:02<00:06, 476MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|█▍    | 965M/3.95G [00:02<00:06, 466MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█▎   | 1.03G/3.95G [00:02<00:06, 483MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█▎   | 1.08G/3.95G [00:02<00:05, 482MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▍   | 1.13G/3.95G [00:02<00:05, 481MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▌   | 1.20G/3.95G [00:02<00:05, 511MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▌   | 1.26G/3.95G [00:02<00:05, 526MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▋   | 1.32G/3.95G [00:02<00:04, 541MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▊   | 1.38G/3.95G [00:02<00:04, 540MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▊   | 1.45G/3.95G [00:03<00:04, 507MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▉   | 1.50G/3.95G [00:03<00:04, 495MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▉   | 1.56G/3.95G [00:03<00:04, 508MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|██   | 1.61G/3.95G [00:03<00:04, 488MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|██   | 1.67G/3.95G [00:03<00:04, 481MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|██▏  | 1.72G/3.95G [00:03<00:04, 479MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|██▏  | 1.77G/3.95G [00:03<00:04, 474MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|██▎  | 1.82G/3.95G [00:03<00:04, 477MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|██▍  | 1.88G/3.95G [00:04<00:04, 476MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|██▍  | 1.93G/3.95G [00:04<00:04, 443MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██▌  | 1.98G/3.95G [00:04<00:04, 445MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██▌  | 2.03G/3.95G [00:04<00:04, 464MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██▋  | 2.09G/3.95G [00:04<00:03, 480MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▋  | 2.14G/3.95G [00:04<00:03, 456MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▊  | 2.19G/3.95G [00:04<00:03, 470MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▊  | 2.24G/3.95G [00:04<00:03, 472MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▉  | 2.30G/3.95G [00:04<00:03, 479MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▉  | 2.36G/3.95G [00:05<00:03, 504MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|███  | 2.41G/3.95G [00:05<00:03, 472MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|███  | 2.46G/3.95G [00:05<00:03, 470MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|███▏ | 2.52G/3.95G [00:05<00:03, 431MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|███▎ | 2.57G/3.95G [00:05<00:03, 430MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|███▎ | 2.63G/3.95G [00:05<00:03, 428MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|███▍ | 2.68G/3.95G [00:05<00:03, 420MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|███▍ | 2.74G/3.95G [00:05<00:02, 428MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|███▌ | 2.80G/3.95G [00:06<00:02, 456MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|███▌ | 2.85G/3.95G [00:06<00:02, 467MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|███▋ | 2.90G/3.95G [00:06<00:02, 468MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███▋ | 2.96G/3.95G [00:06<00:02, 460MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███▊ | 3.01G/3.95G [00:06<00:02, 458MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▉ | 3.06G/3.95G [00:06<00:01, 471MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▉ | 3.11G/3.95G [00:06<00:01, 438MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|████ | 3.18G/3.95G [00:06<00:01, 472MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████ | 3.23G/3.95G [00:06<00:01, 453MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|████▏| 3.28G/3.95G [00:07<00:01, 462MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|████▏| 3.34G/3.95G [00:07<00:01, 490MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|████▎| 3.40G/3.95G [00:07<00:01, 498MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|████▎| 3.45G/3.95G [00:07<00:01, 483MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|████▍| 3.50G/3.95G [00:07<00:00, 483MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|████▌| 3.57G/3.95G [00:07<00:00, 506MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|████▌| 3.63G/3.95G [00:07<00:00, 525MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|████▋| 3.69G/3.95G [00:07<00:00, 480MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|████▋| 3.74G/3.95G [00:08<00:00, 481MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|████▊| 3.80G/3.95G [00:08<00:00, 481MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|████▉| 3.85G/3.95G [00:08<00:00, 479MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|█████| 3.95G/3.95G [00:08<00:00, 468MB/s]\u001b[A\n",
      "Downloading shards:  25%|██████▎                  | 1/4 [00:08<00:25,  8.54s/it]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|     | 52.4M/3.86G [00:00<00:07, 491MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏     | 105M/3.86G [00:00<00:07, 485MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏     | 157M/3.86G [00:00<00:07, 469MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎     | 210M/3.86G [00:00<00:08, 422MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▍     | 262M/3.86G [00:00<00:08, 449MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍     | 315M/3.86G [00:00<00:07, 469MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌     | 377M/3.86G [00:00<00:07, 485MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▋     | 430M/3.86G [00:00<00:07, 490MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▋     | 482M/3.86G [00:01<00:06, 499MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▊     | 535M/3.86G [00:01<00:06, 504MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▉     | 598M/3.86G [00:01<00:06, 514MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|█     | 650M/3.86G [00:01<00:06, 512MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|█     | 703M/3.86G [00:01<00:06, 507MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|█▏    | 755M/3.86G [00:01<00:06, 494MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|█▎    | 818M/3.86G [00:01<00:05, 513MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|█▎    | 870M/3.86G [00:01<00:05, 504MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|█▍    | 923M/3.86G [00:01<00:06, 489MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█▌    | 975M/3.86G [00:01<00:05, 491MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█▎   | 1.03G/3.86G [00:02<00:05, 480MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█▍   | 1.08G/3.86G [00:02<00:06, 451MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▍   | 1.13G/3.86G [00:02<00:05, 457MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▌   | 1.20G/3.86G [00:02<00:05, 488MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▌   | 1.25G/3.86G [00:02<00:05, 476MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▋   | 1.30G/3.86G [00:02<00:05, 450MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▊   | 1.35G/3.86G [00:02<00:06, 392MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▊   | 1.39G/3.86G [00:02<00:06, 390MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▊   | 1.45G/3.86G [00:03<00:05, 412MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▉   | 1.50G/3.86G [00:03<00:05, 431MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|██   | 1.55G/3.86G [00:03<00:05, 432MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|██   | 1.60G/3.86G [00:03<00:05, 434MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|██▏  | 1.66G/3.86G [00:03<00:05, 422MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|██▏  | 1.71G/3.86G [00:03<00:04, 437MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|██▎  | 1.77G/3.86G [00:03<00:04, 462MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|██▎  | 1.84G/3.86G [00:03<00:04, 499MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|██▍  | 1.90G/3.86G [00:04<00:03, 509MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██▌  | 1.95G/3.86G [00:04<00:04, 436MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██▌  | 2.00G/3.86G [00:04<00:04, 447MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██▋  | 2.06G/3.86G [00:04<00:03, 463MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▋  | 2.12G/3.86G [00:04<00:03, 473MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▊  | 2.17G/3.86G [00:04<00:03, 476MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▉  | 2.22G/3.86G [00:04<00:03, 479MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▉  | 2.28G/3.86G [00:04<00:03, 490MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|███  | 2.33G/3.86G [00:04<00:03, 491MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|███  | 2.39G/3.86G [00:05<00:02, 503MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|███▏ | 2.45G/3.86G [00:05<00:02, 513MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|███▏ | 2.51G/3.86G [00:05<00:02, 492MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|███▎ | 2.56G/3.86G [00:05<00:02, 485MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|███▍ | 2.61G/3.86G [00:05<00:02, 470MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|███▍ | 2.66G/3.86G [00:05<00:02, 468MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|███▌ | 2.72G/3.86G [00:05<00:02, 459MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|███▌ | 2.78G/3.86G [00:05<00:02, 474MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|███▋ | 2.83G/3.86G [00:06<00:02, 474MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███▋ | 2.88G/3.86G [00:06<00:03, 322MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███▊ | 2.95G/3.86G [00:06<00:02, 380MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▉ | 3.01G/3.86G [00:06<00:01, 430MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▉ | 3.06G/3.86G [00:06<00:01, 449MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|████ | 3.11G/3.86G [00:06<00:01, 458MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|████ | 3.17G/3.86G [00:06<00:01, 452MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|████▏| 3.22G/3.86G [00:06<00:01, 444MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|████▏| 3.27G/3.86G [00:07<00:01, 463MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|████▎| 3.32G/3.86G [00:07<00:01, 464MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|████▎| 3.38G/3.86G [00:07<00:01, 458MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|████▍| 3.43G/3.86G [00:07<00:00, 455MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|████▌| 3.48G/3.86G [00:07<00:00, 445MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|████▌| 3.53G/3.86G [00:07<00:00, 454MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|████▋| 3.60G/3.86G [00:07<00:00, 478MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|████▋| 3.65G/3.86G [00:07<00:00, 455MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|████▊| 3.70G/3.86G [00:08<00:00, 461MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|████▊| 3.75G/3.86G [00:08<00:00, 438MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|████▉| 3.81G/3.86G [00:08<00:00, 458MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|█████| 3.86G/3.86G [00:08<00:00, 461MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 2/4 [00:17<00:17,  8.50s/it]\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|     | 31.5M/3.86G [00:00<00:12, 311MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|     | 83.9M/3.86G [00:00<00:09, 416MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏     | 136M/3.86G [00:00<00:08, 434MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▎     | 189M/3.86G [00:00<00:08, 458MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎     | 241M/3.86G [00:00<00:08, 442MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍     | 294M/3.86G [00:00<00:07, 450MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▌     | 357M/3.86G [00:00<00:07, 478MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▋     | 409M/3.86G [00:00<00:07, 479MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▋     | 461M/3.86G [00:01<00:07, 464MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▊     | 514M/3.86G [00:01<00:07, 458MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▉     | 566M/3.86G [00:01<00:07, 460MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▉     | 619M/3.86G [00:01<00:07, 426MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|█     | 671M/3.86G [00:01<00:07, 421MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|█     | 724M/3.86G [00:01<00:07, 436MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|█▏    | 776M/3.86G [00:01<00:07, 430MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|█▎    | 828M/3.86G [00:01<00:07, 428MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|█▎    | 881M/3.86G [00:02<00:07, 402MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|█▍    | 923M/3.86G [00:02<00:07, 406MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|█▍    | 965M/3.86G [00:02<00:07, 391MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█▎   | 1.01G/3.86G [00:02<00:07, 385MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█▎   | 1.05G/3.86G [00:02<00:07, 382MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█▍   | 1.10G/3.86G [00:02<00:06, 410MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▍   | 1.14G/3.86G [00:02<00:06, 405MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▌   | 1.18G/3.86G [00:02<00:07, 367MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▌   | 1.23G/3.86G [00:02<00:06, 380MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▋   | 1.28G/3.86G [00:03<00:06, 414MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▋   | 1.33G/3.86G [00:03<00:06, 385MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▊   | 1.38G/3.86G [00:03<00:06, 395MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▊   | 1.44G/3.86G [00:03<00:05, 411MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▉   | 1.48G/3.86G [00:03<00:05, 412MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▉   | 1.52G/3.86G [00:03<00:05, 396MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|██   | 1.56G/3.86G [00:03<00:06, 381MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|██   | 1.61G/3.86G [00:03<00:05, 398MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|██▏  | 1.67G/3.86G [00:04<00:05, 413MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|██▏  | 1.71G/3.86G [00:04<00:05, 380MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|██▎  | 1.76G/3.86G [00:04<00:05, 408MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|██▎  | 1.81G/3.86G [00:04<00:04, 430MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|██▍  | 1.87G/3.86G [00:04<00:04, 403MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|██▍  | 1.93G/3.86G [00:04<00:04, 452MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██▌  | 1.98G/3.86G [00:04<00:04, 451MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██▋  | 2.03G/3.86G [00:04<00:04, 443MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▋  | 2.09G/3.86G [00:04<00:04, 444MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▊  | 2.14G/3.86G [00:05<00:03, 444MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▊  | 2.19G/3.86G [00:05<00:03, 434MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▉  | 2.24G/3.86G [00:05<00:03, 424MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▉  | 2.30G/3.86G [00:05<00:03, 420MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|███  | 2.35G/3.86G [00:05<00:03, 430MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|███  | 2.40G/3.86G [00:05<00:03, 402MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|███▏ | 2.45G/3.86G [00:05<00:03, 428MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|███▏ | 2.51G/3.86G [00:05<00:03, 430MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|███▎ | 2.56G/3.86G [00:06<00:03, 409MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|███▎ | 2.60G/3.86G [00:06<00:03, 376MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|███▍ | 2.64G/3.86G [00:06<00:03, 379MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|███▍ | 2.68G/3.86G [00:06<00:03, 389MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|███▌ | 2.73G/3.86G [00:06<00:03, 368MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|███▌ | 2.78G/3.86G [00:06<00:02, 394MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|███▋ | 2.83G/3.86G [00:06<00:02, 406MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|███▋ | 2.87G/3.86G [00:06<00:02, 392MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███▊ | 2.94G/3.86G [00:07<00:02, 436MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███▊ | 2.99G/3.86G [00:07<00:01, 444MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▉ | 3.04G/3.86G [00:07<00:01, 425MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|████ | 3.09G/3.86G [00:07<00:01, 407MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|████ | 3.14G/3.86G [00:07<00:01, 396MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|████ | 3.18G/3.86G [00:07<00:01, 399MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|████▏| 3.23G/3.86G [00:07<00:01, 415MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|████▏| 3.28G/3.86G [00:07<00:01, 427MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|████▎| 3.33G/3.86G [00:08<00:01, 402MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|████▎| 3.38G/3.86G [00:08<00:01, 395MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|████▍| 3.42G/3.86G [00:08<00:01, 401MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|████▍| 3.47G/3.86G [00:08<00:00, 415MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|████▌| 3.52G/3.86G [00:08<00:00, 425MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|████▋| 3.58G/3.86G [00:08<00:00, 383MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|████▋| 3.63G/3.86G [00:08<00:00, 394MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|████▊| 3.68G/3.86G [00:08<00:00, 411MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|████▊| 3.73G/3.86G [00:09<00:00, 429MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|████▉| 3.79G/3.86G [00:09<00:00, 383MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|█████| 3.86G/3.86G [00:09<00:00, 412MB/s]\u001b[A\n",
      "Downloading shards:  75%|██████████████████▊      | 3/4 [00:26<00:08,  8.94s/it]\n",
      "model-00004-of-00004.safetensors:   0%|             | 0.00/3.56G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   1%|     | 52.4M/3.56G [00:00<00:07, 467MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   3%|▏     | 105M/3.56G [00:00<00:07, 483MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   4%|▎     | 157M/3.56G [00:00<00:07, 431MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   6%|▎     | 210M/3.56G [00:00<00:07, 447MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   7%|▍     | 262M/3.56G [00:00<00:07, 465MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   9%|▌     | 315M/3.56G [00:00<00:06, 473MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  10%|▌     | 367M/3.56G [00:00<00:06, 476MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  12%|▋     | 419M/3.56G [00:00<00:06, 454MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|▊     | 472M/3.56G [00:01<00:06, 459MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  15%|▉     | 524M/3.56G [00:01<00:07, 415MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  16%|▉     | 577M/3.56G [00:01<00:06, 441MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  18%|█     | 629M/3.56G [00:01<00:06, 442MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  19%|█▏    | 682M/3.56G [00:01<00:06, 430MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  21%|█▏    | 734M/3.56G [00:01<00:06, 443MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  22%|█▎    | 786M/3.56G [00:01<00:06, 450MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  24%|█▍    | 839M/3.56G [00:01<00:06, 445MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  25%|█▌    | 891M/3.56G [00:01<00:06, 439MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  27%|█▌    | 944M/3.56G [00:02<00:06, 402MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  28%|█▋    | 996M/3.56G [00:02<00:06, 404MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  29%|█▍   | 1.04G/3.56G [00:02<00:06, 383MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  30%|█▌   | 1.08G/3.56G [00:02<00:06, 387MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  32%|█▌   | 1.12G/3.56G [00:02<00:06, 393MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  33%|█▋   | 1.16G/3.56G [00:02<00:06, 365MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  34%|█▋   | 1.21G/3.56G [00:02<00:06, 363MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  35%|█▊   | 1.25G/3.56G [00:03<00:06, 343MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  37%|█▊   | 1.30G/3.56G [00:03<00:06, 369MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  38%|█▉   | 1.35G/3.56G [00:03<00:05, 398MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  40%|█▉   | 1.41G/3.56G [00:03<00:05, 419MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  41%|██   | 1.46G/3.56G [00:03<00:05, 401MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  42%|██   | 1.51G/3.56G [00:03<00:04, 412MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  44%|██▏  | 1.55G/3.56G [00:03<00:05, 389MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  45%|██▏  | 1.59G/3.56G [00:03<00:05, 383MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  46%|██▎  | 1.64G/3.56G [00:03<00:04, 386MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  47%|██▎  | 1.68G/3.56G [00:04<00:04, 388MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  48%|██▍  | 1.72G/3.56G [00:04<00:04, 374MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  50%|██▍  | 1.76G/3.56G [00:04<00:04, 374MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  51%|██▌  | 1.81G/3.56G [00:04<00:04, 406MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  52%|██▌  | 1.86G/3.56G [00:04<00:04, 399MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  54%|██▋  | 1.91G/3.56G [00:04<00:04, 412MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  55%|██▊  | 1.96G/3.56G [00:04<00:03, 422MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  57%|██▊  | 2.01G/3.56G [00:04<00:03, 423MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  58%|██▉  | 2.07G/3.56G [00:05<00:03, 387MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  59%|██▉  | 2.11G/3.56G [00:05<00:03, 365MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  60%|███  | 2.15G/3.56G [00:05<00:03, 356MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  62%|███  | 2.19G/3.56G [00:05<00:03, 350MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  63%|███▏ | 2.24G/3.56G [00:05<00:03, 385MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  64%|███▏ | 2.29G/3.56G [00:05<00:03, 384MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 2.34G/3.56G [00:05<00:02, 418MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  67%|███▎ | 2.39G/3.56G [00:05<00:02, 432MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  69%|███▍ | 2.44G/3.56G [00:05<00:02, 444MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  70%|███▌ | 2.50G/3.56G [00:06<00:02, 443MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  72%|███▌ | 2.55G/3.56G [00:06<00:02, 434MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  73%|███▋ | 2.60G/3.56G [00:06<00:02, 439MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  75%|███▋ | 2.65G/3.56G [00:06<00:01, 457MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  76%|███▊ | 2.71G/3.56G [00:06<00:01, 474MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  78%|███▉ | 2.76G/3.56G [00:06<00:01, 425MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  79%|███▉ | 2.82G/3.56G [00:06<00:01, 470MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  81%|████ | 2.87G/3.56G [00:06<00:01, 479MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  82%|████ | 2.93G/3.56G [00:07<00:01, 469MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  84%|████▏| 2.98G/3.56G [00:07<00:01, 462MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  86%|████▎| 3.04G/3.56G [00:07<00:01, 482MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  87%|████▎| 3.09G/3.56G [00:07<00:00, 474MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  88%|████▍| 3.15G/3.56G [00:07<00:00, 480MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  90%|████▍| 3.20G/3.56G [00:07<00:00, 449MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  91%|████▌| 3.25G/3.56G [00:07<00:00, 445MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  93%|████▋| 3.30G/3.56G [00:07<00:00, 465MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  95%|████▋| 3.37G/3.56G [00:07<00:00, 486MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  96%|████▊| 3.43G/3.56G [00:08<00:00, 500MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  98%|████▉| 3.48G/3.56G [00:08<00:00, 488MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|█████| 3.56G/3.56G [00:08<00:00, 427MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:34<00:00,  8.73s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "generation_config.json: 100%|██████████████████| 243/243 [00:00<00:00, 4.43MB/s]\n",
      "Loading PEFT: ./qwen2.5-7b-functioncall/checkpoint-900\n",
      "Running merge_and_unload\n",
      "tokenizer_config.json: 100%|███████████████| 7.30k/7.30k [00:00<00:00, 58.7MB/s]\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 8.81MB/s]\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 25.5MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 24.5MB/s]\n",
      "Model saved to ./output_dir\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "peft_model_id = \"qwen2.5-7b-multiturn_fc/checkpoint-900\"\n",
    "\n",
    "!python merge.py \\\n",
    "    --base_model_name_or_path Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --peft_model_path ./qwen2.5-7b-functioncall/checkpoint-900 \\\n",
    "    --output_dir ./output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c9a8bd-e7e8-4b59-b7d3-ca2e9e6498f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: '==2.5.8'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flash_attn ==2.5.8 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47877e8-888a-4019-919e-d60ed9c3964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566d2e11314d493e8abb094a636da77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./output_dir')\n",
    "model = AutoModelForCausalLM.from_pretrained('./output_dir')\n",
    "model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02e5c2e4-9c03-4e5a-bdc5-1db31c594d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":system_prompt +'오늘 날짜는 2025-03-09 입니다.'},\n",
    "    {\"role\": \"user\", \"content\": \"테슬라의 최근 성장률은 어때?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac9e48a-b063-4917-9356-1ca1ac3413b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.decode(model_inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7403a237-7db8-4337-b5c3-e7e9f9457c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 다양한 기능을 호출할 수 있는 AI 모델입니다. 사용자의 요청에 따라 특정 함수를 호출하고, 함수의 시그니처는 <tools></tools> XML 태그 내에 제공됩니다. 함수를 호출할 때는 함수에 필요한 정확한 값을 추정하지 말고 사용자가 제공한 값에 따라 함수를 실행해야 합니다.\n",
      "\n",
      "아래는 사용 가능한 함수들과 각각의 파라미터에 대한 설명입니다:\n",
      "\n",
      "get_issues_summarized\n",
      "\n",
      "설명: 특정 회사 또는 키워드에 대한 이슈를 검색하고 요약합니다.\n",
      "파라미터:\n",
      "keyword: 이슈/현황을 검색하고자 하는 회사명 또는 키워드.\n",
      "days: 검색하고자 하는 기간(일 단위 (integer)).\n",
      "\n",
      "get_reddit_hotissue\n",
      "\n",
      "설명: 금융시장에서 핫한 이슈를 요약합니다.\n",
      "파라미터:\n",
      "days: 검색하고자 하는 기간(일 단위).\n",
      "\n",
      "{name: get_earnings,\n",
      "설명: 기업의 재무재표 또는 현금흐름을 가져오고 분석합니다. 성장률과 같이 이전 년도 데이터가 필요한경우, 이전 년도 데이터도 한번 더 호출하세요\n",
      "파라미터:{symbol: 실적 데이터를 찾고자 하는 기업의 심볼.\n",
      "analysis_type: 분석 유형(growth, profitability, stability, valuation, cashflow, dividend, cost, NA).\n",
      "type_: 데이터 타입(yearly 또는 quarter).\n",
      "year: 데이터를 찾고자 하는 연도 (명시하지 않을경우, 데이터가 존재하는 최근 연도의 데이터를 참조합니다.).\n",
      "quarter: 데이터를 찾고자 하는 분기(명시하지않은 경우 최근 데이터를 조회하도록 -1 을 입력합니다).}}\n",
      "\n",
      "get_consensus\n",
      "\n",
      "설명: 기업의 EPS 컨센서스 데이터 또는 매수/매도/홀드 의견을 가져와서 분석합니다.\n",
      "파라미터:\n",
      "symbol: 데이터를 찾고자 하는 기업의 심볼.\n",
      "year: 데이터를 찾고자 하는 연도.\n",
      "quarter: 데이터를 찾고자 하는 분기.\n",
      "각 함수 호출 시, JSON 객체를 사용하여 함수 이름과 인자들을 <tool_call></tool_call> XML 태그 내에 명시해야 합니다. 함수 호출 예시는 다음과 같습니다:\n",
      "\n",
      "xml\n",
      "Copy\n",
      "<tool_call>\n",
      "{\n",
      "    \"name\": \"get_earnings\",\n",
      "    \"arguments\": {\n",
      "        \"symbol\": \"AAPL\",\n",
      "        \"analysis_type\": \"growth\",\n",
      "        \"type_\": \"yearly\" ,\n",
      "        \"year\": \"2024\",\n",
      "        \"quarter\": \"-1\"\n",
      "    }\n",
      "}\n",
      "</tool_call>\n",
      "각 함수의 인자 값을 정확하게 지정해 주세요. 특히, 연도와 분기를 설정할 때 현재 날짜가 1월이나 2월인 경우, 최근 연도의 데이터를 참조하도록 주의해야 합니다.\n",
      "**주의사항\n",
      "함수를 호출할때를 제외하고 한국어로 대답하세요.\n",
      "\n",
      "오늘 날짜는 2025-03-09 입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "테슬라의 최근 성장률은 어때?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<tool_call>{\"name\":\"get_earnings\",\"arguments\": {\"symbol\": \"TSLA\", \"analysis_type\": \"growth\", \"type_\": \"yearly\", \"year\": \"2024\", \"quarter\": \"-1\"}}</tool_call><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.module.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=2048, eos_token_id=eos_token_id)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a0a125-cb6f-4eea-ae00-4e7e6aa1ca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: —token 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\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login —token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7eb6a32-509e-429e-9b94-b2c4f5cefe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "username = \"irene93\"\n",
    "\n",
    "\n",
    "MODEL_NAME = 'qwen2.5-7b-multiturn_fc_step900'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b4e91a-8fa3-4ae8-b585-819f198e6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "token='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab35e9c-079b-4dea-98da-81b8622270ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dafdc9855ae4fefafbeda03b73fdaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5568343a299745ce987b84e7cc4491bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa6e773623648aab72e0b9eff82e8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7eed0688584cd8b4901e2f73a13055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d8caa142bc434bbbb5311ba79b3155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3b8898a2a54f6ab11ade504dfc04ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/irene93/qwen2.5-7b-multiturn_fc_step900/commit/ac9cfbdc1a14933eb56b73bcd10370161bd24bd4', commit_message='Upload folder using huggingface_hub', commit_description='', oid='ac9cfbdc1a14933eb56b73bcd10370161bd24bd4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/irene93/qwen2.5-7b-multiturn_fc_step900', endpoint='https://huggingface.co', repo_type='model', repo_id='irene93/qwen2.5-7b-multiturn_fc_step900'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(\n",
    "    token=token, ### 토큰값 ,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    token=token,### 토큰값 ,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path=\"output_dir\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc5823-35d1-48c9-acdc-bde2e302e642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
