{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07984c3e-df7c-4d64-b0d3-c1c5ce07a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd  /workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6aabc4-f461-439d-b695-98e249f81e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "with open(f'./data/final.pkl' , 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5239f2bd-90e1-499f-acfa-adacae5329de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.8.93)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (6.30.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.45.1 in /usr/local/lib/python3.10/dist-packages (4.45.1)\n",
      "Requirement already satisfied: datasets==3.0.1 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
      "Requirement already satisfied: evaluate==0.4.3 in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: bitsandbytes==0.44.0 in /usr/local/lib/python3.10/dist-packages (0.44.0)\n",
      "Requirement already satisfied: trl==0.11.1 in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
      "Requirement already satisfied: peft==0.13.0 in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
      "Requirement already satisfied: qwen-vl-utils in /usr/local/lib/python3.10/dist-packages (0.0.10)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (3.11.13)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.2.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.1) (0.9.17)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (14.2.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (9.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.8.93)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (4.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch==2.2.0\" tensorboard pillow\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a385b3d9-feee-4a77-97cf-e1be9ea0a9b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.7.4.post1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.2.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# GPUê°€ Flash Attentionì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸\n",
    "assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "\n",
    "# Flash Attention ì„¤ì¹˜\n",
    "!pip install ninja packaging\n",
    "!pip install flash-attn --no-build-isolation --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81d57dcc-2f92-4cca-965a-c92f9208c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” AI ëª¨ë¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ íŠ¹ì • í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê³ , í•¨ìˆ˜ì˜ ì‹œê·¸ë‹ˆì²˜ëŠ” <tools></tools> XML íƒœê·¸ ë‚´ì— ì œê³µë©ë‹ˆë‹¤. í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•ŒëŠ” í•¨ìˆ˜ì— í•„ìš”í•œ ì •í™•í•œ ê°’ì„ ì¶”ì •í•˜ì§€ ë§ê³  ì‚¬ìš©ìê°€ ì œê³µí•œ ê°’ì— ë”°ë¼ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤ê³¼ ê°ê°ì˜ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤:\n",
    "\n",
    "get_issues_summarized\n",
    "\n",
    "ì„¤ëª…: íŠ¹ì • íšŒì‚¬ ë˜ëŠ” í‚¤ì›Œë“œì— ëŒ€í•œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.\n",
    "íŒŒë¼ë¯¸í„°:\n",
    "keyword: ì´ìŠˆ/í˜„í™©ì„ ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” íšŒì‚¬ëª… ë˜ëŠ” í‚¤ì›Œë“œ.\n",
    "days: ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” ê¸°ê°„(ì¼ ë‹¨ìœ„ (integer)).\n",
    "\n",
    "get_reddit_hotissue\n",
    "\n",
    "ì„¤ëª…: ê¸ˆìœµì‹œì¥ì—ì„œ í•«í•œ ì´ìŠˆë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.\n",
    "íŒŒë¼ë¯¸í„°:\n",
    "days: ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” ê¸°ê°„(ì¼ ë‹¨ìœ„).\n",
    "\n",
    "{name: get_earnings,\n",
    "ì„¤ëª…: ê¸°ì—…ì˜ ì¬ë¬´ì¬í‘œ ë˜ëŠ” í˜„ê¸ˆíë¦„ì„ ê°€ì ¸ì˜¤ê³  ë¶„ì„í•©ë‹ˆë‹¤. ì„±ì¥ë¥ ê³¼ ê°™ì´ ì´ì „ ë…„ë„ ë°ì´í„°ê°€ í•„ìš”í•œê²½ìš°, ì´ì „ ë…„ë„ ë°ì´í„°ë„ í•œë²ˆ ë” í˜¸ì¶œí•˜ì„¸ìš”\n",
    "íŒŒë¼ë¯¸í„°:{symbol: ì‹¤ì  ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ê¸°ì—…ì˜ ì‹¬ë³¼.\n",
    "analysis_type: ë¶„ì„ ìœ í˜•(growth, profitability, stability, valuation, cashflow, dividend, cost, NA).\n",
    "type_: ë°ì´í„° íƒ€ì…(yearly ë˜ëŠ” quarter).\n",
    "year: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ì—°ë„ (ëª…ì‹œí•˜ì§€ ì•Šì„ê²½ìš°, ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ìµœê·¼ ì—°ë„ì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.).\n",
    "quarter: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ë¶„ê¸°(ëª…ì‹œí•˜ì§€ì•Šì€ ê²½ìš° ìµœê·¼ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ë„ë¡ -1 ì„ ì…ë ¥í•©ë‹ˆë‹¤).}}\n",
    "\n",
    "get_consensus\n",
    "\n",
    "ì„¤ëª…: ê¸°ì—…ì˜ EPS ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ë˜ëŠ” ë§¤ìˆ˜/ë§¤ë„/í™€ë“œ ì˜ê²¬ì„ ê°€ì ¸ì™€ì„œ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "íŒŒë¼ë¯¸í„°:\n",
    "symbol: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ê¸°ì—…ì˜ ì‹¬ë³¼.\n",
    "year: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ì—°ë„.\n",
    "quarter: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ë¶„ê¸°.\n",
    "ê° í•¨ìˆ˜ í˜¸ì¶œ ì‹œ, JSON ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ ì´ë¦„ê³¼ ì¸ìë“¤ì„ <tool_call></tool_call> XML íƒœê·¸ ë‚´ì— ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. í•¨ìˆ˜ í˜¸ì¶œ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "xml\n",
    "Copy\n",
    "<tool_call>\n",
    "{\n",
    "    \"name\": \"get_earnings\",\n",
    "    \"arguments\": {\n",
    "        \"symbol\": \"AAPL\",\n",
    "        \"analysis_type\": \"growth\",\n",
    "        \"type_\": \"yearly\" ,\n",
    "        \"year\": \"2024\",\n",
    "        \"quarter\": \"-1\"\n",
    "    }\n",
    "}\n",
    "</tool_call>\n",
    "ê° í•¨ìˆ˜ì˜ ì¸ì ê°’ì„ ì •í™•í•˜ê²Œ ì§€ì •í•´ ì£¼ì„¸ìš”. íŠ¹íˆ, ì—°ë„ì™€ ë¶„ê¸°ë¥¼ ì„¤ì •í•  ë•Œ í˜„ì¬ ë‚ ì§œê°€ 1ì›”ì´ë‚˜ 2ì›”ì¸ ê²½ìš°, ìµœê·¼ ì—°ë„ì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "**ì£¼ì˜ì‚¬í•­\n",
    "í•¨ìˆ˜ë¥¼ í˜¸ì¶œí• ë•Œë¥¼ ì œì™¸í•˜ê³  í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a423c46-9261-40cb-801d-53bca5c3fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messeges = [{'message':d} for d in data]\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(messeges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e224a3b-9f73-44a5-8755-dc97c089bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list í˜•íƒœì˜ datasetì„ Dataset ê°ì²´ë¡œ ë³€í™˜\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def format_data(example):\n",
    "\n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    if len(example)<3:\n",
    "    \n",
    "        return {'messages':[{\n",
    "                \"content\": system_prompt,\n",
    "                \"role\": \"system\"\n",
    "            },\n",
    "                example[0],\n",
    "                example[1]]\n",
    "               }\n",
    "    else:\n",
    "        return {'messages':[{\n",
    "                \"content\": system_prompt,\n",
    "                \"role\": \"system\"\n",
    "            },\n",
    "                example[0],\n",
    "                example[1],\n",
    "                {'content':example[2]['content'][:4000]+'</tool_response>','role':'user'},\n",
    "                example[3]]\n",
    "               }\n",
    "            \n",
    "    \n",
    "# Apply the formatting on dataset\n",
    "dataset = [format_data(sample) for sample in data]\n",
    "\n",
    "dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64215a02-a644-4dec-b77a-d773dcc6f79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 863\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480cfe44-918e-4df4-8022-055efe24e78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3a307632fc48fabddd6bea413cb1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ID\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\" \n",
    "\n",
    "# BitsAndBytes 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,                             # 4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©\n",
    "   bnb_4bit_use_double_quant=True,               # ì´ì¤‘ ì–‘ìí™” ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½\n",
    "   bnb_4bit_quant_type=\"nf4\",                    # 4ë¹„íŠ¸ ì–‘ìí™” íƒ€ì… ì„¤ì •(normalized float 4)\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16         # ì—°ì‚° ì‹œ bfloat16 íƒ€ì… ì‚¬ìš©\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2680af-7446-4e99-a258-d952ac34458d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” AI ëª¨ë¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ íŠ¹ì • í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê³ , í•¨ìˆ˜ì˜ ì‹œê·¸ë‹ˆì²˜ëŠ” <tools></tools> XML íƒœê·¸ ë‚´ì— ì œê³µë©ë‹ˆë‹¤. í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•ŒëŠ” í•¨ìˆ˜ì— í•„ìš”í•œ ì •í™•í•œ ê°’ì„ ì¶”ì •í•˜ì§€ ë§ê³  ì‚¬ìš©ìê°€ ì œê³µí•œ ê°’ì— ë”°ë¼ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì•„ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤ê³¼ ê°ê°ì˜ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤:\n",
      "\n",
      "get_issues_summarized\n",
      "\n",
      "ì„¤ëª…: íŠ¹ì • íšŒì‚¬ ë˜ëŠ” í‚¤ì›Œë“œì— ëŒ€í•œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.\n",
      "íŒŒë¼ë¯¸í„°:\n",
      "keyword: ì´ìŠˆ/í˜„í™©ì„ ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” íšŒì‚¬ëª… ë˜ëŠ” í‚¤ì›Œë“œ.\n",
      "days: ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” ê¸°ê°„(ì¼ ë‹¨ìœ„ (integer)).\n",
      "\n",
      "get_reddit_hotissue\n",
      "\n",
      "ì„¤ëª…: ê¸ˆìœµì‹œì¥ì—ì„œ í•«í•œ ì´ìŠˆë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.\n",
      "íŒŒë¼ë¯¸í„°:\n",
      "days: ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” ê¸°ê°„(ì¼ ë‹¨ìœ„).\n",
      "\n",
      "{name: get_earnings,\n",
      "ì„¤ëª…: ê¸°ì—…ì˜ ì¬ë¬´ì¬í‘œ ë˜ëŠ” í˜„ê¸ˆíë¦„ì„ ê°€ì ¸ì˜¤ê³  ë¶„ì„í•©ë‹ˆë‹¤. ì„±ì¥ë¥ ê³¼ ê°™ì´ ì´ì „ ë…„ë„ ë°ì´í„°ê°€ í•„ìš”í•œê²½ìš°, ì´ì „ ë…„ë„ ë°ì´í„°ë„ í•œë²ˆ ë” í˜¸ì¶œí•˜ì„¸ìš”\n",
      "íŒŒë¼ë¯¸í„°:{symbol: ì‹¤ì  ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ê¸°ì—…ì˜ ì‹¬ë³¼.\n",
      "analysis_type: ë¶„ì„ ìœ í˜•(growth, profitability, stability, valuation, cashflow, dividend, cost, NA).\n",
      "type_: ë°ì´í„° íƒ€ì…(yearly ë˜ëŠ” quarter).\n",
      "year: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ì—°ë„ (ëª…ì‹œí•˜ì§€ ì•Šì„ê²½ìš°, ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ìµœê·¼ ì—°ë„ì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.).\n",
      "quarter: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ë¶„ê¸°(ëª…ì‹œí•˜ì§€ì•Šì€ ê²½ìš° ìµœê·¼ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ë„ë¡ -1 ì„ ì…ë ¥í•©ë‹ˆë‹¤).}}\n",
      "\n",
      "get_consensus\n",
      "\n",
      "ì„¤ëª…: ê¸°ì—…ì˜ EPS ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ë˜ëŠ” ë§¤ìˆ˜/ë§¤ë„/í™€ë“œ ì˜ê²¬ì„ ê°€ì ¸ì™€ì„œ ë¶„ì„í•©ë‹ˆë‹¤.\n",
      "íŒŒë¼ë¯¸í„°:\n",
      "symbol: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ê¸°ì—…ì˜ ì‹¬ë³¼.\n",
      "year: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ì—°ë„.\n",
      "quarter: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ë¶„ê¸°.\n",
      "ê° í•¨ìˆ˜ í˜¸ì¶œ ì‹œ, JSON ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ ì´ë¦„ê³¼ ì¸ìë“¤ì„ <tool_call></tool_call> XML íƒœê·¸ ë‚´ì— ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. í•¨ìˆ˜ í˜¸ì¶œ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "xml\n",
      "Copy\n",
      "<tool_call>\n",
      "{\n",
      "    \"name\": \"get_earnings\",\n",
      "    \"arguments\": {\n",
      "        \"symbol\": \"AAPL\",\n",
      "        \"analysis_type\": \"growth\",\n",
      "        \"type_\": \"yearly\" ,\n",
      "        \"year\": \"2024\",\n",
      "        \"quarter\": \"-1\"\n",
      "    }\n",
      "}\n",
      "</tool_call>\n",
      "ê° í•¨ìˆ˜ì˜ ì¸ì ê°’ì„ ì •í™•í•˜ê²Œ ì§€ì •í•´ ì£¼ì„¸ìš”. íŠ¹íˆ, ì—°ë„ì™€ ë¶„ê¸°ë¥¼ ì„¤ì •í•  ë•Œ í˜„ì¬ ë‚ ì§œê°€ 1ì›”ì´ë‚˜ 2ì›”ì¸ ê²½ìš°, ìµœê·¼ ì—°ë„ì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "**ì£¼ì˜ì‚¬í•­\n",
      "í•¨ìˆ˜ë¥¼ í˜¸ì¶œí• ë•Œë¥¼ ì œì™¸í•˜ê³  í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.\n",
      "\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "ìµœê·¼ ì¼ì£¼ì¼ ë™ì•ˆ ê¸ˆìœµ ì‹œì¥ì—ì„œ ì–´ë–¤ ì´ìŠˆë“¤ì´ í•«í–ˆë‚˜ìš”?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<tool_call>{\"name\":\"get_reddit_hotissue\",\"arguments\": {\"days\": 7}}</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "<tool_reponse>title :What Are Your Moves Tomorrow, March 10, 2025\n",
      "body :This post contains content not supported on old Reddit. [Click here to view the full post](https://sh.reddit.com/r/wallstreetbets/comments/1j7g4c1)\n",
      "\n",
      "title :Weekly Earnings Thread 3/10 - 3/14\n",
      "body :\n",
      "\n",
      "title :ğŸ¤·\n",
      "body :\n",
      "\n",
      "title :For those wondering why we really need  yields to drop.\n",
      "body :\n",
      "\n",
      "title :+39k trade, 40k->100k in 2 months, as a senior in college\n",
      "body :\n",
      "\n",
      "title :Lost 8k YTD thanks to the trade war. Sold all my SOXL\n",
      "body :Silver lining is that I made 38K last year on SOXL and TQQQ because I sold some shares at the peek in June 2024, roughly half portfolio at the time.  My made  little money on CCâ€™s but not nearly enough to make up for the hemorrhaging. \n",
      "\n",
      "title :Saving the market with my puts.\n",
      "body :Biggest yolo of my life. And I never make money on options. \n",
      "\n",
      "title :US Fund Flows: US Investors Pump Billions into Active ETFs Amid Shaky Market Start in 2025\n",
      "body :\n",
      "\n",
      "title :US car payment delinquencies reach 33-year high: Analysis\n",
      "body :\n",
      "\n",
      "title :GRRR - Gorilla technology group - A 'deep' dive into\n",
      "body :https://preview.redd.it/9y63pv35ione1.png?width=870&format=png&auto=webp&s=12f4f27b42603aa177a784a495d24d16d0eeae77\n",
      "\n",
      "Hey everyone, I'm sharing this DD because, compared to other analyses I've seen, there are some key differences and divergences. This is based on my own research, and I wanted to provide a more complete perspective on Gorilla Technology (GRRR) based on what I found . Iâ€™m just a regular small investor (not a financial advisor), currently holding 1,200 shares along with call options ahead of their webinar. Iâ€™ve spent a significant amount of time digging into their background, SEC filings, and the controversy surrounding short-seller allegations. If Iâ€™ve missed anything or if someone has a different take, Iâ€™d be happy to discuss it.\n",
      "\n",
      "# Is this an AI-generated post?\n",
      "\n",
      "https://preview.redd.it/qenkz99k0pne1.png?width=2433&format=png&auto=webp&s=663e356896e5e3b91f1177f67f541a5bac3d6cda\n",
      "\n",
      "https://preview.redd.it/zpxw7ysu0pne1.png?width=2496&format=png&auto=webp&s=738d68ee81e93eb571caae590636c4f33603698a\n",
      "\n",
      "Many of you in the comments are suggesting that this was AI-generated. While I can say that I spent a lot of time writing and revising it (especially since English isnâ€™t my first language), youâ€™ll never have proof of that. What I *can* show you, however, are some of the methods I use to conduct my analyses. And yes, I used my LLM to format the textâ€” < typical indent used, because who wants to read a long, poorly structured post? I mean, even I wouldnâ€™t want to read my own post again like that.\n",
      "\n",
      "# What Does Gorilla Technology Do?\n",
      "\n",
      "Gorilla operates at the intersection of AI, Industrial IoT, and cybersecurity, providing AI-driven solutions for smart cities and security analytics. Their platforms power video surveillance, facial recognition, network security, and IoT deployments. They work across Asia, the Middle East, Europe, and Latin America.\n",
      "\n",
      "Recent MoUs (memorandums of understanding) indicate massive growth potential, including a $1.8B Thai electric-grid modernization project and a large smart government contract in Egypt. While MoUs arenâ€™t finalized deals, they show strong business momentum.\n",
      "\n",
      "On March 3, **The Bear Cave**â€”a research firm that digs up short ideasâ€”released a note raising what they called â€œcautionary flagsâ€ about Gorilla Technology. They highlighted Gorillaâ€™s roughly **1,200%** stock price jump over six months, pointing to the hype around a series of deals and MoUs (Memorandums of Understanding) that might not be fully locked in. The Bear Cave basically argued that investor excitement might be getting ahead of real fundamentals, noting things like Gorillaâ€™s Cayman Islands registration, workforce distribution (a lot in Taiwan), and its pivot into AI under CEO and Chairman **Jay Chandan**.\n",
      "\n",
      "Naturally, short-selling activity popped up around the same time. But high short interest doesnâ€™t automatically mean the short thesis is correct; it just</tool_response><|im_end|>\n",
      "<|im_start|>assistant\n",
      "ìµœê·¼ ì¼ì£¼ì¼ ë™ì•ˆ ê¸ˆìœµ ì‹œì¥ì—ì„œëŠ” ì—¬ëŸ¬ ì£¼ëª©í•  ë§Œí•œ ì´ìŠˆì™€ ì‚¬ê±´ë“¤ì´ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ìƒìœ„ë¬´ì—­ ê¸°ì¡°**: ë¯¸êµ­ íˆ¬ììë“¤ì€ 2025ë…„ ì´ˆ ì‹œì¥ì´ ë¶ˆì•ˆì •í•œ ìƒí™©ì—ì„œ ì ê·¹ì ìœ¼ë¡œ ì•¡í‹°ë¸Œ ìƒì¥ì§€ìˆ˜í€ë“œ(ETF)ì— ìê¸ˆì„ íˆ¬ì…í•˜ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ë¯¸êµ­ ìë™ì°¨ ëŒ€ì¶œ ì—°ì²´ìœ¨**: ë¯¸êµ­ ë‚´ ìë™ì°¨ ëŒ€ì¶œ ì—°ì²´ìœ¨ì´ 33ë…„ ë§Œì— ìµœê³ ì¹˜ë¥¼ ê¸°ë¡í–ˆë‹¤ëŠ” ë¶„ì„ì´ ë‚˜ì˜¤ë©° ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ê³ ë¦´ë¼ í…Œí¬ë†€ë¡œì§€(GRRR)**: ê³ ë¦´ë¼ í…Œí¬ë†€ë¡œì§€ê°€ ìµœê·¼ MOU ì²´ê²°ê³¼ ê´€ë ¨ëœ ë…¼ë€ ì†ì—ì„œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ íƒœêµ­ì˜ ì „ê¸° ê·¸ë¦¬ë“œ í˜„ëŒ€í™” í”„ë¡œì íŠ¸ì™€ ì´ì§‘íŠ¸ì˜ ìŠ¤ë§ˆíŠ¸ ì •ë¶€ ê³„ì•½ê³¼ ê´€ë ¨ëœ ë©”ëª¨ëœë¤ ì–¸ê¸‰ì´ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ë¯¸êµ­ ë‚´ ê¸°ì—… ìˆ˜ìµ ë³´ê³ **: ë‹¤ì–‘í•œ ê¸°ì—…ì˜ ìµœê·¼ ìˆ˜ìµ ë³´ê³ ê°€ ìˆì—ˆìœ¼ë©°, íˆ¬ììë“¤ì€ ì‹œì¥ì˜ ë³€ë™ì„±ì„ ì£¼ì˜ ê¹Šê²Œ ì‚´í”¼ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ë ˆë”§ ì£¼ì‹ ì´ìŠˆ**: $RDDTë¡œ ë¶ˆë¦¬ëŠ” ë ˆë”§ ì£¼ì‹ì´ ìµœê·¼ ì‹œì¥ì—ì„œ ìƒë‹¹í•œ ê°€ê²© í•˜ë½ì„ ê²ªê³  ìˆëŠ”ë°, ì´ëŠ” ì£¼ë¡œ ì§€ì •í•™ì  ë¬¸ì œì™€ ê´€ë ¨ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ° ì´ìŠˆë“¤ì€ ì‹œì¥ì˜ ë³€ë™ì„±ê³¼ íˆ¬ì íŒë‹¨ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì£¼ìš” ìš”ì†Œë“¤ë¡œ, íˆ¬ììë“¤ì€ ì´ëŸ¬í•œ ì •ë³´ë“¤ì„ ê³ ë ¤í•˜ì—¬ íˆ¬ì ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•  ê²ƒì…ë‹ˆë‹¤.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# í…œí”Œë¦¿ ì ìš©\n",
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7a88eb-c0af-42a5-a857-bb0cc4cf78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    " \n",
    "# QLoRA ë…¼ë¬¸ ë° Sebastian Raschka ì‹¤í—˜ì— ê¸°ë°˜í•œ LoRA Conifg\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# from transformers import TrainingArguments\n",
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2.5-7b-functioncall\",     # ì €ì¥ë  ë””ë ‰í† ë¦¬ì™€ ì €ì¥ì†Œ ID\n",
    "    num_train_epochs=10,                      # í•™ìŠµí•  ì´ ì—í¬í¬ ìˆ˜\n",
    "    per_device_train_batch_size=8,           # ì¥ì¹˜ë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°\n",
    "    gradient_accumulation_steps=2,           # ì—­ì „íŒŒ/ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì „ ëˆ„ì í•  ìŠ¤í… ìˆ˜\n",
    "    gradient_checkpointing=True,             # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©\n",
    "    optim=\"adamw_torch_fused\",               # í“¨ì¦ˆë“œ AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "    logging_steps=10,                        # 10ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ê¸°ë¡\n",
    "    save_strategy=\"steps\",                   # íŠ¹ì • ìŠ¤í…ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "    save_steps=50,\n",
    "    bf16=True,                              # bfloat16 ì •ë°€ë„ ì‚¬ìš©\n",
    "    tf32=True,                              # tf32 ì •ë°€ë„ ì‚¬ìš©\n",
    "    learning_rate=1e-4,                     # í•™ìŠµë¥  (QLoRA ë…¼ë¬¸ ê¸°ë°˜)\n",
    "    max_grad_norm=0.3,                      # ìµœëŒ€ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ (QLoRA ë…¼ë¬¸ ê¸°ë°˜)\n",
    "    warmup_ratio=0.03,                      # ì›Œë°ì—… ë¹„ìœ¨ (QLoRA ë…¼ë¬¸ ê¸°ë°˜)\n",
    "    lr_scheduler_type=\"constant\",           # ê³ ì • í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©\n",
    "    push_to_hub=False,                      # í—ˆë¸Œì— ëª¨ë¸ ì—…ë¡œë“œ ì•ˆ í•¨\n",
    "    report_to=\"tensorboard\",                # í…ì„œë³´ë“œì— ë©”íŠ¸ë¦­ ê¸°ë¡\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4b993f-2e3c-4412-b04d-6cf6f838a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        # messagesì˜ ê° ë‚´ìš©ì—ì„œ ê°œí–‰ë¬¸ì ì œê±°\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        # ê¹¨ë—í•´ì§„ ë©”ì‹œì§€ë¡œ í…œí”Œë¦¿ ì ìš©\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # ë ˆì´ë¸” ì´ˆê¸°í™”\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # assistant ì‘ë‹µ ë¶€ë¶„ ì°¾ê¸°\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        # í† í° ID ê°€ì ¸ì˜¤ê¸°\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # <|im_start|>assistant ì°¾ê¸°\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                # assistant í† í° ì°¾ê¸°\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    # assistant ì‘ë‹µì˜ ì‹œì‘ ìœ„ì¹˜ë¡œ ì´ë™\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    # <|im_end|>ë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ë ˆì´ë¸” ì„¤ì •\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "                            # <|im_end|> í† í°ë„ ë ˆì´ë¸”ì— í¬í•¨\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    # íŒ¨ë”© ì ìš©\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    # í…ì„œë¡œ ë³€í™˜\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch\n",
    "\n",
    "def print_tokens_and_labels(batch):\n",
    "    input_ids = batch[\"input_ids\"][0].tolist()\n",
    "    labels = batch[\"labels\"][0].tolist()\n",
    "    \n",
    "    print(\"\\ní† í°ê³¼ ë ˆì´ë¸” ë¹„êµ:\")\n",
    "    print(f\"{'Token ID':<10} {'Token':<30} {'Label':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for token_id, label in zip(input_ids, labels):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        label_str = str(label) if label != -100 else \"-100\"\n",
    "        print(f\"{token_id:<10} {token:<30} {label_str:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc830518-ccff-4b01-9436-d77d6bc07725",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70e8172e-d095-46dc-ac8b-a46b98e61393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì²˜ë¦¬ëœ ë°°ì¹˜ ë°ì´í„°:\n",
      "ì…ë ¥ ID í˜•íƒœ: torch.Size([1, 2200])\n",
      "ì–´í…ì…˜ ë§ˆìŠ¤í¬ í˜•íƒœ: torch.Size([1, 2200])\n",
      "ë ˆì´ë¸” í˜•íƒœ: torch.Size([1, 2200])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn í…ŒìŠ¤íŠ¸ (ë°°ì¹˜ í¬ê¸° 1ë¡œ)\n",
    "max_seq_length = 8192  \n",
    "batch = collate_fn([example])\n",
    "print(\"\\nì²˜ë¦¬ëœ ë°°ì¹˜ ë°ì´í„°:\")\n",
    "print(\"ì…ë ¥ ID í˜•íƒœ:\", batch[\"input_ids\"].shape)\n",
    "print(\"ì–´í…ì…˜ ë§ˆìŠ¤í¬ í˜•íƒœ:\", batch[\"attention_mask\"].shape)\n",
    "print(\"ë ˆì´ë¸” í˜•íƒœ:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8890e5a5-c8c5-492f-98fe-bac9787733a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 151657, 4913, 606, 3252, 455, 1288, 20090, 33433, 11159, 2198, 16370, 788, 5212, 13778, 788, 220, 22, 3417, 151658, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 128215, 125722, 83556, 54330, 32077, 126322, 126246, 40771, 230, 128024, 44518, 40853, 129889, 127296, 55673, 87608, 47836, 62107, 23573, 23084, 144018, 80573, 138629, 126253, 35339, 127750, 382, 16, 13, 3070, 55902, 80901, 125054, 126346, 54116, 92817, 95518, 132662, 10764, 230, 105, 25715, 25715, 128901, 220, 17, 15, 17, 20, 126216, 83315, 44518, 40853, 12802, 126488, 126246, 29281, 23573, 58034, 130803, 56475, 135968, 131529, 128552, 23872, 94, 131131, 131196, 58034, 40853, 21329, 23259, 144419, 29346, 7, 42239, 8, 19391, 64577, 125052, 17877, 10764, 230, 105, 43866, 126204, 64521, 132091, 135513, 138097, 38231, 382, 17, 13, 3070, 56039, 124785, 64577, 57089, 125625, 60960, 69923, 77353, 49543, 132841, 95518, 132662, 66136, 64577, 57089, 125625, 60960, 69923, 77353, 49543, 132841, 12802, 220, 18, 18, 126216, 62107, 19391, 81173, 34395, 59698, 18411, 54116, 49664, 128836, 16560, 128618, 129150, 12802, 137298, 124905, 55673, 87608, 132872, 34395, 128472, 382, 18, 13, 3070, 34395, 135379, 50340, 10764, 72509, 81133, 137666, 17380, 21329, 6699, 8106, 49, 32295, 25, 126429, 135379, 50340, 10764, 72509, 81133, 137666, 17380, 21329, 19969, 139465, 11418, 52, 48364, 112, 88781, 53680, 129985, 52300, 127041, 120, 129804, 77596, 235, 56475, 55673, 87608, 132872, 34395, 128472, 13, 136115, 74361, 250, 124785, 20401, 56419, 20487, 126342, 29346, 141526, 66845, 56290, 84255, 17380, 144644, 28626, 80573, 23084, 126886, 28626, 20401, 141767, 125544, 28626, 36055, 63089, 94203, 125535, 53680, 129985, 52300, 51391, 129439, 136499, 144452, 139957, 128911, 12802, 35339, 127750, 382, 19, 13, 3070, 56039, 124785, 66136, 54116, 124517, 28733, 131870, 63332, 34395, 95518, 135392, 54116, 124517, 20401, 139465, 28733, 131870, 63332, 34395, 19969, 132236, 127378, 11, 10764, 230, 105, 25715, 25715, 128901, 44518, 40853, 20401, 46319, 57089, 132818, 55673, 20401, 130507, 232, 57801, 127166, 129262, 34395, 128472, 382, 20, 13, 3070, 126673, 144934, 55673, 76337, 23084, 144018, 95518, 400, 97068, 51, 17380, 126488, 132920, 5140, 254, 230, 144934, 55673, 76337, 12802, 139465, 44518, 40853, 56475, 58034, 64795, 23573, 35509, 126614, 53900, 132772, 17877, 23894, 103, 34395, 134563, 11, 23084, 16560, 55673, 17380, 66790, 29281, 124632, 80968, 126674, 80573, 129985, 12802, 64521, 132091, 63332, 78952, 382, 12802, 125120, 23084, 144018, 128901, 44518, 40853, 20401, 46319, 57089, 32831, 53680, 10764, 230, 105, 25715, 140569, 19391, 126440, 129321, 17877, 125714, 142588, 28733, 64521, 55673, 35711, 85997, 43590, 64850, 17380, 11, 10764, 230, 105, 25715, 25715, 128901, 131367, 60039, 129125, 126429, 125476, 82190, 10764, 230, 105, 25715, 56419, 138279, 17877, 28733, 126702, 129264, 95002, 130882, 13, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('ì…ë ¥ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa8673c-ff0b-4e6c-a495-d6ef174da621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 8192  # ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ íŒ¨í‚¹ì„ ìœ„í•œ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725a44cd-f178-49dc-bca2-6dfbfa197f42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# í•™ìŠµ ì‹œì‘\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain(resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2.5-7b-functioncall/checkpoint-500\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í—ˆë¸Œì™€ output_dirì— ì €ì¥ë¨\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ëª¨ë¸ ì €ì¥\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()   \u001b[38;5;66;03m# ìµœì¢… ëª¨ë¸ì„ ì €ì¥\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ì‹œì‘\n",
    "trainer.train(resume_from_checkpoint=\"qwen2.5-7b-functioncall/checkpoint-500\")   # ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í—ˆë¸Œì™€ output_dirì— ì €ì¥ë¨\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model()   # ìµœì¢… ëª¨ë¸ì„ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f2c08b-6737-49bb-84c2-2eb925dad7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-09 11:20:19--  https://raw.githubusercontent.com/ukairia777/LLM-Finetuning-tutorial/main/merge.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "Length: 1351 (1.3K) [text/plain]\n",
      "Saving to: â€˜merge.pyâ€™\n",
      "\n",
      "merge.py            100%[===================>]   1.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-09 11:20:19 (69.6 MB/s) - â€˜merge.pyâ€™ saved [1351/1351]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ukairia777/LLM-Finetuning-tutorial/main/merge.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c28c5a8a-b143-4258-8a5a-64d24b427881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d68dd06-77e7-448f-8ef9-982c06a95000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2\n",
      "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2) (12.1.105)\n",
      "Collecting triton==2.2.0 (from torch==2.2)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2) (1.3.0)\n",
      "Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 torch-2.2.0 triton-2.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "142ca8d2-d627-4f05-b52e-829a8e1b761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Qwen/Qwen2.5-7B-Instruct\n",
      "model.safetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27.8k/27.8k [00:00<00:00, 56.5MB/s]\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/3.95G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|     | 31.5M/3.95G [00:00<00:17, 226MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 62.9M/3.95G [00:00<00:15, 244MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 94.4M/3.95G [00:00<00:15, 247MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|â–     | 126M/3.95G [00:00<00:15, 244MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|â–     | 157M/3.95G [00:00<00:16, 234MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|â–     | 189M/3.95G [00:00<00:16, 232MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|â–     | 220M/3.95G [00:00<00:15, 236MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|â–     | 252M/3.95G [00:01<00:15, 233MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|â–     | 283M/3.95G [00:01<00:16, 227MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|â–     | 315M/3.95G [00:01<00:15, 230MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|â–Œ     | 346M/3.95G [00:01<00:15, 235MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|â–Œ     | 377M/3.95G [00:01<00:14, 244MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|â–Œ     | 409M/3.95G [00:01<00:14, 240MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|â–‹     | 440M/3.95G [00:01<00:14, 240MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|â–‹     | 472M/3.95G [00:01<00:14, 238MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|â–Š     | 503M/3.95G [00:02<00:14, 237MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|â–Š     | 535M/3.95G [00:02<00:14, 240MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|â–Š     | 566M/3.95G [00:02<00:14, 240MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|â–‰     | 598M/3.95G [00:02<00:13, 243MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|â–‰     | 629M/3.95G [00:02<00:13, 243MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|â–ˆ     | 661M/3.95G [00:02<00:13, 247MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|â–ˆ     | 692M/3.95G [00:02<00:13, 247MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|â–ˆ     | 724M/3.95G [00:03<00:13, 245MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|â–ˆâ–    | 755M/3.95G [00:03<00:13, 245MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|â–ˆâ–    | 786M/3.95G [00:03<00:13, 242MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|â–ˆâ–    | 818M/3.95G [00:03<00:12, 242MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|â–ˆâ–    | 849M/3.95G [00:03<00:14, 219MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|â–ˆâ–    | 881M/3.95G [00:03<00:13, 221MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|â–ˆâ–    | 912M/3.95G [00:03<00:14, 207MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|â–ˆâ–    | 944M/3.95G [00:04<00:13, 215MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|â–ˆâ–    | 975M/3.95G [00:04<00:13, 220MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|â–ˆâ–   | 1.01G/3.95G [00:04<00:15, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|â–ˆâ–   | 1.03G/3.95G [00:04<00:15, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|â–ˆâ–   | 1.06G/3.95G [00:04<00:13, 208MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|â–ˆâ–   | 1.09G/3.95G [00:04<00:13, 216MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|â–ˆâ–   | 1.12G/3.95G [00:04<00:12, 223MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|â–ˆâ–   | 1.15G/3.95G [00:05<00:12, 224MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|â–ˆâ–Œ   | 1.18G/3.95G [00:05<00:12, 227MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|â–ˆâ–Œ   | 1.22G/3.95G [00:05<00:12, 225MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|â–ˆâ–Œ   | 1.25G/3.95G [00:05<00:11, 233MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|â–ˆâ–Œ   | 1.28G/3.95G [00:05<00:13, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|â–ˆâ–‹   | 1.31G/3.95G [00:05<00:12, 211MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|â–ˆâ–‹   | 1.34G/3.95G [00:05<00:11, 221MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|â–ˆâ–‹   | 1.37G/3.95G [00:06<00:11, 227MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|â–ˆâ–Š   | 1.41G/3.95G [00:06<00:10, 232MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|â–ˆâ–Š   | 1.44G/3.95G [00:06<00:10, 237MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|â–ˆâ–Š   | 1.47G/3.95G [00:06<00:13, 178MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|â–ˆâ–‰   | 1.50G/3.95G [00:06<00:12, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|â–ˆâ–‰   | 1.53G/3.95G [00:06<00:11, 205MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|â–ˆâ–‰   | 1.56G/3.95G [00:06<00:10, 217MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|â–ˆâ–ˆ   | 1.59G/3.95G [00:07<00:10, 228MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|â–ˆâ–ˆ   | 1.63G/3.95G [00:07<00:10, 229MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|â–ˆâ–ˆ   | 1.66G/3.95G [00:07<00:09, 232MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|â–ˆâ–ˆâ–  | 1.69G/3.95G [00:07<00:12, 180MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|â–ˆâ–ˆâ–  | 1.72G/3.95G [00:07<00:11, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|â–ˆâ–ˆâ–  | 1.75G/3.95G [00:07<00:10, 209MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|â–ˆâ–ˆâ–  | 1.78G/3.95G [00:07<00:10, 216MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|â–ˆâ–ˆâ–  | 1.81G/3.95G [00:08<00:09, 221MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|â–ˆâ–ˆâ–  | 1.85G/3.95G [00:08<00:09, 227MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|â–ˆâ–ˆâ–  | 1.88G/3.95G [00:08<00:08, 230MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|â–ˆâ–ˆâ–  | 1.91G/3.95G [00:08<00:11, 180MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|â–ˆâ–ˆâ–  | 1.94G/3.95G [00:08<00:10, 195MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|â–ˆâ–ˆâ–  | 1.97G/3.95G [00:08<00:10, 191MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 2.00G/3.95G [00:09<00:09, 202MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 2.03G/3.95G [00:09<00:09, 211MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 2.07G/3.95G [00:09<00:08, 221MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|â–ˆâ–ˆâ–‹  | 2.10G/3.95G [00:09<00:09, 193MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|â–ˆâ–ˆâ–‹  | 2.13G/3.95G [00:09<00:08, 205MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|â–ˆâ–ˆâ–‹  | 2.16G/3.95G [00:09<00:08, 219MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|â–ˆâ–ˆâ–Š  | 2.19G/3.95G [00:09<00:07, 224MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|â–ˆâ–ˆâ–Š  | 2.22G/3.95G [00:10<00:07, 227MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|â–ˆâ–ˆâ–Š  | 2.25G/3.95G [00:10<00:07, 230MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|â–ˆâ–ˆâ–‰  | 2.29G/3.95G [00:10<00:07, 233MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|â–ˆâ–ˆâ–‰  | 2.32G/3.95G [00:10<00:08, 182MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|â–ˆâ–ˆâ–‰  | 2.35G/3.95G [00:10<00:08, 194MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|â–ˆâ–ˆâ–ˆ  | 2.38G/3.95G [00:10<00:07, 208MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 2.41G/3.95G [00:11<00:07, 210MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 2.44G/3.95G [00:11<00:06, 220MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.47G/3.95G [00:11<00:06, 227MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 2.51G/3.95G [00:11<00:06, 230MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 2.54G/3.95G [00:11<00:07, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|â–ˆâ–ˆâ–ˆâ– | 2.57G/3.95G [00:11<00:06, 200MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|â–ˆâ–ˆâ–ˆâ– | 2.60G/3.95G [00:11<00:06, 211MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|â–ˆâ–ˆâ–ˆâ– | 2.63G/3.95G [00:12<00:06, 216MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.66G/3.95G [00:12<00:05, 221MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.69G/3.95G [00:12<00:05, 228MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 2.73G/3.95G [00:12<00:06, 182MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|â–ˆâ–ˆâ–ˆâ– | 2.76G/3.95G [00:12<00:06, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.79G/3.95G [00:12<00:05, 207MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.82G/3.95G [00:12<00:05, 212MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 2.85G/3.95G [00:13<00:04, 220MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 2.88G/3.95G [00:13<00:04, 222MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 2.92G/3.95G [00:13<00:04, 226MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–‹ | 2.95G/3.95G [00:13<00:05, 186MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–Š | 2.98G/3.95G [00:13<00:04, 198MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 3.01G/3.95G [00:13<00:04, 187MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 3.04G/3.95G [00:14<00:04, 203MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 3.07G/3.95G [00:14<00:04, 212MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 3.10G/3.95G [00:14<00:03, 218MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 3.14G/3.95G [00:14<00:03, 205MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.17G/3.95G [00:14<00:04, 190MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.20G/3.95G [00:14<00:03, 204MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.23G/3.95G [00:14<00:03, 211MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.26G/3.95G [00:15<00:03, 219MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.29G/3.95G [00:15<00:02, 226MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.32G/3.95G [00:15<00:02, 228MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.36G/3.95G [00:15<00:02, 197MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.39G/3.95G [00:15<00:02, 209MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.42G/3.95G [00:15<00:02, 217MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.45G/3.95G [00:15<00:02, 226MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.48G/3.95G [00:16<00:02, 231MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.51G/3.95G [00:16<00:01, 237MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.54G/3.95G [00:16<00:01, 238MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.58G/3.95G [00:16<00:02, 182MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.61G/3.95G [00:16<00:01, 200MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.64G/3.95G [00:16<00:01, 212MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.67G/3.95G [00:16<00:01, 221MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.70G/3.95G [00:17<00:01, 221MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.73G/3.95G [00:17<00:00, 224MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.76G/3.95G [00:17<00:00, 233MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.80G/3.95G [00:17<00:00, 178MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.83G/3.95G [00:17<00:00, 195MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.86G/3.95G [00:17<00:00, 208MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.89G/3.95G [00:18<00:00, 219MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.92G/3.95G [00:18<00:00, 224MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.95G/3.95G [00:18<00:00, 216MB/s]\u001b[A\n",
      "Downloading shards:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 1/4 [00:18<00:55, 18.44s/it]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|     | 21.0M/3.86G [00:00<00:21, 176MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|     | 41.9M/3.86G [00:00<00:19, 194MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|     | 73.4M/3.86G [00:00<00:17, 215MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|â–     | 105M/3.86G [00:00<00:16, 223MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|â–     | 136M/3.86G [00:00<00:16, 226MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|â–     | 168M/3.86G [00:00<00:16, 225MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|â–     | 199M/3.86G [00:00<00:15, 231MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|â–     | 231M/3.86G [00:01<00:15, 232MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|â–     | 262M/3.86G [00:01<00:15, 235MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|â–     | 294M/3.86G [00:01<00:15, 235MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|â–Œ     | 325M/3.86G [00:01<00:14, 240MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|â–Œ     | 357M/3.86G [00:01<00:14, 244MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|â–Œ     | 388M/3.86G [00:01<00:14, 246MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|â–‹     | 419M/3.86G [00:01<00:14, 246MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|â–‹     | 451M/3.86G [00:01<00:14, 244MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|â–‹     | 482M/3.86G [00:02<00:14, 236MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|â–Š     | 514M/3.86G [00:02<00:14, 234MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|â–Š     | 545M/3.86G [00:02<00:13, 238MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|â–‰     | 577M/3.86G [00:02<00:14, 225MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|â–‰     | 608M/3.86G [00:02<00:14, 228MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|â–‰     | 640M/3.86G [00:02<00:13, 234MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|â–ˆ     | 671M/3.86G [00:03<00:18, 168MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|â–ˆ     | 703M/3.86G [00:03<00:16, 188MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|â–ˆâ–    | 734M/3.86G [00:03<00:15, 203MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|â–ˆâ–    | 765M/3.86G [00:03<00:14, 214MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|â–ˆâ–    | 797M/3.86G [00:03<00:13, 219MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|â–ˆâ–    | 828M/3.86G [00:03<00:13, 225MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|â–ˆâ–    | 860M/3.86G [00:03<00:13, 230MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|â–ˆâ–    | 891M/3.86G [00:03<00:12, 240MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|â–ˆâ–    | 923M/3.86G [00:04<00:12, 240MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|â–ˆâ–    | 954M/3.86G [00:04<00:13, 223MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|â–ˆâ–Œ    | 986M/3.86G [00:04<00:12, 236MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|â–ˆâ–   | 1.02G/3.86G [00:04<00:11, 240MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|â–ˆâ–   | 1.05G/3.86G [00:04<00:11, 240MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|â–ˆâ–   | 1.08G/3.86G [00:04<00:11, 241MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|â–ˆâ–   | 1.11G/3.86G [00:04<00:11, 233MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|â–ˆâ–   | 1.14G/3.86G [00:05<00:11, 235MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|â–ˆâ–Œ   | 1.17G/3.86G [00:05<00:11, 237MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|â–ˆâ–Œ   | 1.21G/3.86G [00:05<00:11, 233MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|â–ˆâ–Œ   | 1.24G/3.86G [00:05<00:11, 236MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|â–ˆâ–‹   | 1.27G/3.86G [00:05<00:10, 245MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|â–ˆâ–‹   | 1.30G/3.86G [00:05<00:10, 243MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|â–ˆâ–‹   | 1.33G/3.86G [00:05<00:10, 242MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|â–ˆâ–Š   | 1.36G/3.86G [00:05<00:10, 242MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|â–ˆâ–Š   | 1.39G/3.86G [00:06<00:10, 244MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|â–ˆâ–Š   | 1.43G/3.86G [00:06<00:09, 245MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|â–ˆâ–‰   | 1.46G/3.86G [00:06<00:09, 242MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|â–ˆâ–‰   | 1.49G/3.86G [00:06<00:10, 235MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|â–ˆâ–‰   | 1.52G/3.86G [00:06<00:09, 237MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|â–ˆâ–ˆ   | 1.55G/3.86G [00:06<00:09, 239MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|â–ˆâ–ˆ   | 1.58G/3.86G [00:06<00:09, 239MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|â–ˆâ–ˆ   | 1.61G/3.86G [00:06<00:09, 239MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|â–ˆâ–ˆâ–  | 1.65G/3.86G [00:07<00:09, 239MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|â–ˆâ–ˆâ–  | 1.68G/3.86G [00:07<00:09, 235MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|â–ˆâ–ˆâ–  | 1.71G/3.86G [00:07<00:09, 239MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|â–ˆâ–ˆâ–  | 1.74G/3.86G [00:07<00:08, 240MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|â–ˆâ–ˆâ–  | 1.77G/3.86G [00:07<00:08, 238MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|â–ˆâ–ˆâ–  | 1.80G/3.86G [00:07<00:09, 226MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|â–ˆâ–ˆâ–  | 1.84G/3.86G [00:07<00:08, 232MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|â–ˆâ–ˆâ–  | 1.87G/3.86G [00:08<00:08, 234MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|â–ˆâ–ˆâ–  | 1.90G/3.86G [00:08<00:09, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|â–ˆâ–ˆâ–  | 1.93G/3.86G [00:08<00:09, 211MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 1.96G/3.86G [00:08<00:08, 220MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 1.99G/3.86G [00:08<00:08, 226MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 2.02G/3.86G [00:08<00:08, 228MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|â–ˆâ–ˆâ–‹  | 2.06G/3.86G [00:08<00:07, 234MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|â–ˆâ–ˆâ–‹  | 2.09G/3.86G [00:09<00:07, 235MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|â–ˆâ–ˆâ–‹  | 2.12G/3.86G [00:09<00:09, 184MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|â–ˆâ–ˆâ–Š  | 2.15G/3.86G [00:09<00:08, 199MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|â–ˆâ–ˆâ–Š  | 2.18G/3.86G [00:09<00:08, 210MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|â–ˆâ–ˆâ–Š  | 2.21G/3.86G [00:09<00:07, 217MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|â–ˆâ–ˆâ–‰  | 2.24G/3.86G [00:09<00:07, 225MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|â–ˆâ–ˆâ–‰  | 2.28G/3.86G [00:09<00:06, 235MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|â–ˆâ–ˆâ–‰  | 2.31G/3.86G [00:10<00:08, 179MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|â–ˆâ–ˆâ–ˆ  | 2.33G/3.86G [00:10<00:08, 176MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 2.35G/3.86G [00:10<00:08, 182MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 2.38G/3.86G [00:10<00:07, 198MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 2.41G/3.86G [00:10<00:06, 211MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.44G/3.86G [00:10<00:06, 221MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 2.47G/3.86G [00:10<00:06, 226MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|â–ˆâ–ˆâ–ˆâ– | 2.51G/3.86G [00:11<00:05, 227MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|â–ˆâ–ˆâ–ˆâ– | 2.54G/3.86G [00:11<00:06, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|â–ˆâ–ˆâ–ˆâ– | 2.57G/3.86G [00:11<00:06, 213MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|â–ˆâ–ˆâ–ˆâ– | 2.60G/3.86G [00:11<00:05, 219MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.63G/3.86G [00:11<00:05, 219MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 2.66G/3.86G [00:11<00:05, 225MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|â–ˆâ–ˆâ–ˆâ– | 2.69G/3.86G [00:11<00:05, 229MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.73G/3.86G [00:12<00:06, 185MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.76G/3.86G [00:12<00:05, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 2.79G/3.86G [00:12<00:05, 214MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 2.82G/3.86G [00:12<00:04, 220MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 2.85G/3.86G [00:12<00:04, 230MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–‹ | 2.88G/3.86G [00:12<00:04, 233MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–Š | 2.92G/3.86G [00:12<00:04, 235MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 2.95G/3.86G [00:13<00:05, 178MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 2.98G/3.86G [00:13<00:04, 195MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 3.01G/3.86G [00:13<00:04, 212MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 3.04G/3.86G [00:13<00:03, 220MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 3.07G/3.86G [00:13<00:03, 223MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.10G/3.86G [00:13<00:03, 228MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.14G/3.86G [00:14<00:03, 233MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.17G/3.86G [00:14<00:04, 167MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.20G/3.86G [00:14<00:03, 184MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.23G/3.86G [00:14<00:03, 199MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.26G/3.86G [00:14<00:02, 211MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.29G/3.86G [00:14<00:02, 219MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.32G/3.86G [00:15<00:02, 226MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.36G/3.86G [00:15<00:02, 190MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.39G/3.86G [00:15<00:02, 207MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.42G/3.86G [00:15<00:02, 204MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.45G/3.86G [00:15<00:01, 218MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.48G/3.86G [00:15<00:01, 224MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.51G/3.86G [00:15<00:01, 227MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.54G/3.86G [00:16<00:01, 232MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.58G/3.86G [00:16<00:01, 186MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.61G/3.86G [00:16<00:01, 201MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.64G/3.86G [00:16<00:01, 214MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.67G/3.86G [00:16<00:00, 222MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.70G/3.86G [00:16<00:00, 229MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.73G/3.86G [00:16<00:00, 234MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.76G/3.86G [00:17<00:00, 236MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.80G/3.86G [00:17<00:00, 178MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.83G/3.86G [00:17<00:00, 193MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.86G/3.86G [00:17<00:00, 220MB/s]\u001b[A\n",
      "Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 2/4 [00:36<00:35, 17.98s/it]\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|     | 31.5M/3.86G [00:00<00:15, 240MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|     | 62.9M/3.86G [00:00<00:15, 242MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|     | 94.4M/3.86G [00:00<00:15, 241MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|â–     | 126M/3.86G [00:00<00:15, 243MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|â–     | 157M/3.86G [00:00<00:15, 246MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|â–     | 189M/3.86G [00:00<00:15, 238MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|â–     | 220M/3.86G [00:00<00:15, 238MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|â–     | 252M/3.86G [00:01<00:14, 243MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|â–     | 283M/3.86G [00:01<00:14, 250MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|â–     | 315M/3.86G [00:01<00:14, 246MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|â–Œ     | 346M/3.86G [00:01<00:14, 244MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|â–Œ     | 377M/3.86G [00:01<00:14, 241MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|â–‹     | 409M/3.86G [00:01<00:14, 244MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|â–‹     | 440M/3.86G [00:01<00:14, 243MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|â–‹     | 472M/3.86G [00:01<00:14, 241MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|â–Š     | 503M/3.86G [00:02<00:13, 251MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|â–Š     | 535M/3.86G [00:02<00:13, 250MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|â–‰     | 566M/3.86G [00:02<00:13, 251MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|â–‰     | 598M/3.86G [00:02<00:13, 250MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|â–‰     | 629M/3.86G [00:02<00:13, 247MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|â–ˆ     | 661M/3.86G [00:02<00:12, 249MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|â–ˆ     | 692M/3.86G [00:02<00:12, 251MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|â–ˆ     | 724M/3.86G [00:02<00:12, 250MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|â–ˆâ–    | 755M/3.86G [00:03<00:12, 246MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|â–ˆâ–    | 786M/3.86G [00:03<00:12, 244MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|â–ˆâ–    | 818M/3.86G [00:03<00:14, 215MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|â–ˆâ–    | 849M/3.86G [00:03<00:16, 188MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|â–ˆâ–    | 881M/3.86G [00:03<00:14, 205MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|â–ˆâ–    | 912M/3.86G [00:03<00:13, 213MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|â–ˆâ–    | 944M/3.86G [00:03<00:13, 224MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|â–ˆâ–Œ    | 975M/3.86G [00:04<00:12, 229MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|â–ˆâ–   | 1.01G/3.86G [00:04<00:11, 239MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|â–ˆâ–   | 1.04G/3.86G [00:04<00:11, 239MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|â–ˆâ–   | 1.07G/3.86G [00:04<00:15, 179MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|â–ˆâ–   | 1.10G/3.86G [00:04<00:14, 194MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|â–ˆâ–   | 1.13G/3.86G [00:04<00:13, 210MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|â–ˆâ–Œ   | 1.16G/3.86G [00:05<00:12, 221MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|â–ˆâ–Œ   | 1.20G/3.86G [00:05<00:11, 227MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|â–ˆâ–Œ   | 1.23G/3.86G [00:05<00:11, 233MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|â–ˆâ–‹   | 1.26G/3.86G [00:05<00:14, 174MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|â–ˆâ–‹   | 1.29G/3.86G [00:05<00:13, 188MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|â–ˆâ–‹   | 1.32G/3.86G [00:05<00:12, 202MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|â–ˆâ–Š   | 1.35G/3.86G [00:05<00:12, 207MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|â–ˆâ–Š   | 1.38G/3.86G [00:06<00:11, 222MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|â–ˆâ–Š   | 1.42G/3.86G [00:06<00:10, 229MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|â–ˆâ–Š   | 1.45G/3.86G [00:06<00:10, 238MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|â–ˆâ–‰   | 1.48G/3.86G [00:06<00:13, 182MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|â–ˆâ–‰   | 1.51G/3.86G [00:06<00:11, 197MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|â–ˆâ–‰   | 1.54G/3.86G [00:06<00:11, 209MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|â–ˆâ–ˆ   | 1.57G/3.86G [00:07<00:10, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|â–ˆâ–ˆ   | 1.60G/3.86G [00:07<00:10, 224MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|â–ˆâ–ˆ   | 1.64G/3.86G [00:07<00:09, 230MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|â–ˆâ–ˆâ–  | 1.67G/3.86G [00:07<00:09, 236MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|â–ˆâ–ˆâ–  | 1.70G/3.86G [00:07<00:12, 178MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|â–ˆâ–ˆâ–  | 1.73G/3.86G [00:07<00:10, 194MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|â–ˆâ–ˆâ–  | 1.76G/3.86G [00:07<00:09, 212MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|â–ˆâ–ˆâ–  | 1.79G/3.86G [00:08<00:09, 221MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|â–ˆâ–ˆâ–  | 1.82G/3.86G [00:08<00:09, 221MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|â–ˆâ–ˆâ–  | 1.86G/3.86G [00:08<00:08, 226MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|â–ˆâ–ˆâ–  | 1.89G/3.86G [00:08<00:11, 179MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|â–ˆâ–ˆâ–  | 1.92G/3.86G [00:08<00:10, 192MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|â–ˆâ–ˆâ–Œ  | 1.95G/3.86G [00:08<00:09, 204MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 1.98G/3.86G [00:08<00:08, 212MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 2.01G/3.86G [00:09<00:11, 160MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|â–ˆâ–ˆâ–‹  | 2.04G/3.86G [00:09<00:10, 181MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|â–ˆâ–ˆâ–‹  | 2.08G/3.86G [00:09<00:09, 194MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|â–ˆâ–ˆâ–‹  | 2.11G/3.86G [00:09<00:08, 206MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|â–ˆâ–ˆâ–Š  | 2.14G/3.86G [00:09<00:07, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|â–ˆâ–ˆâ–Š  | 2.17G/3.86G [00:09<00:07, 220MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|â–ˆâ–ˆâ–Š  | 2.20G/3.86G [00:10<00:07, 225MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|â–ˆâ–ˆâ–‰  | 2.23G/3.86G [00:10<00:07, 223MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|â–ˆâ–ˆâ–‰  | 2.26G/3.86G [00:10<00:06, 230MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|â–ˆâ–ˆâ–‰  | 2.30G/3.86G [00:10<00:06, 229MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|â–ˆâ–ˆâ–ˆ  | 2.33G/3.86G [00:10<00:07, 208MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 2.36G/3.86G [00:10<00:06, 225MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 2.39G/3.86G [00:10<00:06, 228MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.42G/3.86G [00:11<00:06, 228MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.45G/3.86G [00:11<00:06, 228MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 2.49G/3.86G [00:11<00:06, 230MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|â–ˆâ–ˆâ–ˆâ– | 2.52G/3.86G [00:11<00:07, 183MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|â–ˆâ–ˆâ–ˆâ– | 2.55G/3.86G [00:11<00:06, 195MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|â–ˆâ–ˆâ–ˆâ– | 2.58G/3.86G [00:11<00:06, 205MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.61G/3.86G [00:11<00:05, 217MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.64G/3.86G [00:12<00:05, 225MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 2.67G/3.86G [00:12<00:05, 228MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–Œ | 2.71G/3.86G [00:12<00:05, 232MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.74G/3.86G [00:12<00:06, 184MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 2.77G/3.86G [00:12<00:05, 195MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 2.80G/3.86G [00:12<00:05, 207MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 2.83G/3.86G [00:13<00:04, 214MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 2.86G/3.86G [00:13<00:04, 220MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–‹ | 2.89G/3.86G [00:13<00:04, 229MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 2.93G/3.86G [00:13<00:04, 209MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 2.96G/3.86G [00:13<00:04, 190MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 2.99G/3.86G [00:13<00:04, 200MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 3.02G/3.86G [00:13<00:04, 209MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 3.05G/3.86G [00:14<00:03, 217MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–‰ | 3.08G/3.86G [00:14<00:03, 222MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.11G/3.86G [00:14<00:03, 225MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.15G/3.86G [00:14<00:03, 185MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 3.18G/3.86G [00:14<00:03, 199MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.21G/3.86G [00:14<00:03, 209MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.24G/3.86G [00:14<00:02, 213MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.27G/3.86G [00:15<00:02, 231MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.30G/3.86G [00:15<00:02, 233MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.33G/3.86G [00:15<00:02, 234MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.37G/3.86G [00:15<00:02, 182MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.40G/3.86G [00:15<00:02, 197MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.43G/3.86G [00:15<00:02, 208MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.46G/3.86G [00:16<00:01, 216MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.49G/3.86G [00:16<00:01, 222MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.52G/3.86G [00:16<00:01, 229MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.55G/3.86G [00:16<00:01, 230MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.59G/3.86G [00:16<00:01, 188MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.62G/3.86G [00:16<00:01, 201MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.65G/3.86G [00:16<00:01, 212MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.68G/3.86G [00:17<00:00, 190MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.70G/3.86G [00:17<00:00, 194MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.73G/3.86G [00:17<00:00, 205MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.76G/3.86G [00:17<00:00, 225MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.80G/3.86G [00:17<00:00, 195MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.83G/3.86G [00:17<00:00, 211MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.86G/3.86G [00:17<00:00, 215MB/s]\u001b[A\n",
      "Downloading shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 3/4 [00:54<00:17, 17.99s/it]\n",
      "model-00004-of-00004.safetensors:   0%|             | 0.00/3.56G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   1%|     | 21.0M/3.56G [00:00<00:24, 143MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   1%|     | 52.4M/3.56G [00:00<00:18, 190MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   2%|     | 83.9M/3.56G [00:00<00:16, 211MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   3%|â–     | 115M/3.56G [00:00<00:15, 224MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   4%|â–     | 147M/3.56G [00:00<00:14, 228MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   5%|â–     | 178M/3.56G [00:00<00:14, 233MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   6%|â–     | 210M/3.56G [00:00<00:14, 232MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   7%|â–     | 241M/3.56G [00:01<00:14, 231MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   8%|â–     | 273M/3.56G [00:01<00:14, 234MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   9%|â–Œ     | 304M/3.56G [00:01<00:13, 235MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   9%|â–Œ     | 336M/3.56G [00:01<00:13, 238MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  10%|â–Œ     | 367M/3.56G [00:01<00:13, 239MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  11%|â–‹     | 398M/3.56G [00:01<00:13, 240MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  12%|â–‹     | 430M/3.56G [00:01<00:13, 231MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|â–Š     | 461M/3.56G [00:02<00:13, 231MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  14%|â–Š     | 493M/3.56G [00:02<00:13, 234MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  15%|â–‰     | 524M/3.56G [00:02<00:12, 236MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  16%|â–‰     | 556M/3.56G [00:02<00:12, 233MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  17%|â–‰     | 587M/3.56G [00:02<00:12, 235MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  17%|â–ˆ     | 619M/3.56G [00:02<00:12, 235MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  18%|â–ˆ     | 650M/3.56G [00:02<00:12, 233MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  19%|â–ˆâ–    | 682M/3.56G [00:02<00:12, 235MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  20%|â–ˆâ–    | 713M/3.56G [00:03<00:11, 238MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  21%|â–ˆâ–    | 744M/3.56G [00:03<00:11, 240MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  22%|â–ˆâ–    | 776M/3.56G [00:03<00:11, 244MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  23%|â–ˆâ–    | 807M/3.56G [00:03<00:11, 240MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  24%|â–ˆâ–    | 839M/3.56G [00:03<00:11, 246MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  24%|â–ˆâ–    | 870M/3.56G [00:03<00:10, 245MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  25%|â–ˆâ–Œ    | 902M/3.56G [00:03<00:11, 241MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  26%|â–ˆâ–Œ    | 933M/3.56G [00:03<00:10, 240MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  27%|â–ˆâ–‹    | 965M/3.56G [00:04<00:10, 239MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  28%|â–ˆâ–‹    | 996M/3.56G [00:04<00:10, 235MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  29%|â–ˆâ–   | 1.03G/3.56G [00:04<00:10, 237MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  30%|â–ˆâ–   | 1.06G/3.56G [00:04<00:12, 200MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  31%|â–ˆâ–Œ   | 1.09G/3.56G [00:04<00:11, 207MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  32%|â–ˆâ–Œ   | 1.12G/3.56G [00:04<00:11, 215MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  32%|â–ˆâ–Œ   | 1.15G/3.56G [00:05<00:10, 221MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  33%|â–ˆâ–‹   | 1.18G/3.56G [00:05<00:10, 227MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  34%|â–ˆâ–‹   | 1.22G/3.56G [00:05<00:10, 228MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  35%|â–ˆâ–Š   | 1.25G/3.56G [00:05<00:09, 233MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  36%|â–ˆâ–Š   | 1.28G/3.56G [00:05<00:12, 188MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  37%|â–ˆâ–Š   | 1.31G/3.56G [00:05<00:11, 203MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  38%|â–ˆâ–‰   | 1.34G/3.56G [00:05<00:10, 212MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  39%|â–ˆâ–‰   | 1.37G/3.56G [00:06<00:09, 221MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  40%|â–ˆâ–‰   | 1.41G/3.56G [00:06<00:09, 221MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  40%|â–ˆâ–ˆ   | 1.44G/3.56G [00:06<00:09, 232MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  41%|â–ˆâ–ˆ   | 1.47G/3.56G [00:06<00:11, 181MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  42%|â–ˆâ–ˆ   | 1.50G/3.56G [00:06<00:10, 195MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  43%|â–ˆâ–ˆâ–  | 1.53G/3.56G [00:06<00:09, 206MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  44%|â–ˆâ–ˆâ–  | 1.56G/3.56G [00:06<00:09, 216MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  45%|â–ˆâ–ˆâ–  | 1.59G/3.56G [00:07<00:08, 223MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  46%|â–ˆâ–ˆâ–  | 1.63G/3.56G [00:07<00:08, 222MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  47%|â–ˆâ–ˆâ–  | 1.66G/3.56G [00:07<00:08, 216MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  47%|â–ˆâ–ˆâ–  | 1.69G/3.56G [00:07<00:10, 187MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  48%|â–ˆâ–ˆâ–  | 1.72G/3.56G [00:07<00:09, 196MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  49%|â–ˆâ–ˆâ–  | 1.75G/3.56G [00:07<00:08, 208MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  50%|â–ˆâ–ˆâ–Œ  | 1.78G/3.56G [00:08<00:08, 218MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  51%|â–ˆâ–ˆâ–Œ  | 1.81G/3.56G [00:08<00:07, 223MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  52%|â–ˆâ–ˆâ–Œ  | 1.85G/3.56G [00:08<00:08, 213MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  53%|â–ˆâ–ˆâ–‹  | 1.88G/3.56G [00:08<00:07, 220MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  54%|â–ˆâ–ˆâ–‹  | 1.91G/3.56G [00:08<00:08, 193MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  55%|â–ˆâ–ˆâ–‹  | 1.94G/3.56G [00:08<00:07, 206MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  55%|â–ˆâ–ˆâ–Š  | 1.97G/3.56G [00:08<00:07, 218MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  56%|â–ˆâ–ˆâ–Š  | 2.00G/3.56G [00:09<00:06, 223MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  57%|â–ˆâ–ˆâ–Š  | 2.03G/3.56G [00:09<00:06, 228MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  58%|â–ˆâ–ˆâ–‰  | 2.07G/3.56G [00:09<00:06, 231MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  59%|â–ˆâ–ˆâ–‰  | 2.10G/3.56G [00:09<00:08, 179MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  60%|â–ˆâ–ˆâ–‰  | 2.13G/3.56G [00:09<00:07, 198MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  61%|â–ˆâ–ˆâ–ˆ  | 2.16G/3.56G [00:09<00:06, 207MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  62%|â–ˆâ–ˆâ–ˆ  | 2.19G/3.56G [00:09<00:06, 218MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.22G/3.56G [00:10<00:05, 224MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  63%|â–ˆâ–ˆâ–ˆâ– | 2.25G/3.56G [00:10<00:05, 229MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  64%|â–ˆâ–ˆâ–ˆâ– | 2.29G/3.56G [00:10<00:05, 233MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  65%|â–ˆâ–ˆâ–ˆâ– | 2.32G/3.56G [00:10<00:06, 180MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|â–ˆâ–ˆâ–ˆâ– | 2.35G/3.56G [00:10<00:06, 195MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  67%|â–ˆâ–ˆâ–ˆâ– | 2.38G/3.56G [00:10<00:05, 211MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  68%|â–ˆâ–ˆâ–ˆâ– | 2.41G/3.56G [00:10<00:05, 216MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  69%|â–ˆâ–ˆâ–ˆâ– | 2.44G/3.56G [00:11<00:05, 223MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  70%|â–ˆâ–ˆâ–ˆâ– | 2.47G/3.56G [00:11<00:04, 229MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–Œ | 2.51G/3.56G [00:11<00:04, 233MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  71%|â–ˆâ–ˆâ–ˆâ–Œ | 2.54G/3.56G [00:11<00:05, 180MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–Œ | 2.57G/3.56G [00:11<00:05, 194MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  73%|â–ˆâ–ˆâ–ˆâ–‹ | 2.60G/3.56G [00:11<00:04, 204MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 2.63G/3.56G [00:12<00:04, 212MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–‹ | 2.66G/3.56G [00:12<00:04, 222MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 2.69G/3.56G [00:12<00:03, 228MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–Š | 2.73G/3.56G [00:12<00:04, 181MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 2.76G/3.56G [00:12<00:04, 199MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 2.79G/3.56G [00:12<00:03, 208MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–‰ | 2.82G/3.56G [00:12<00:03, 216MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.85G/3.56G [00:13<00:03, 222MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.88G/3.56G [00:13<00:02, 231MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.92G/3.56G [00:13<00:02, 217MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 2.95G/3.56G [00:13<00:03, 184MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 2.98G/3.56G [00:13<00:02, 195MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.01G/3.56G [00:13<00:02, 208MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.04G/3.56G [00:14<00:02, 215MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.07G/3.56G [00:14<00:02, 223MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.10G/3.56G [00:14<00:01, 230MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.14G/3.56G [00:14<00:01, 229MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.17G/3.56G [00:14<00:02, 183MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.20G/3.56G [00:14<00:01, 198MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.23G/3.56G [00:14<00:01, 209MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.26G/3.56G [00:15<00:01, 219MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.29G/3.56G [00:15<00:01, 223MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.32G/3.56G [00:15<00:01, 228MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.36G/3.56G [00:15<00:01, 182MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.39G/3.56G [00:15<00:00, 196MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.42G/3.56G [00:15<00:00, 207MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.45G/3.56G [00:15<00:00, 218MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.48G/3.56G [00:16<00:00, 225MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.51G/3.56G [00:16<00:00, 220MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.56G/3.56G [00:16<00:00, 217MB/s]\u001b[A\n",
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:10<00:00, 17.65s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:17<00:00,  4.27s/it]\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243/243 [00:00<00:00, 4.65MB/s]\n",
      "Loading PEFT: ./qwen2.5-7b-functioncall/checkpoint-500\n",
      "Running merge_and_unload\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.30k/7.30k [00:00<00:00, 57.3MB/s]\n",
      "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.78M/2.78M [00:00<00:00, 16.6MB/s]\n",
      "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.67M/1.67M [00:00<00:00, 12.8MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.03M/7.03M [00:00<00:00, 10.4MB/s]\n",
      "Model saved to ./output_dir\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "peft_model_id = \"qwen2.5-7b-functioncall/checkpoint-500\"\n",
    "\n",
    "!python merge.py \\\n",
    "    --base_model_name_or_path Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --peft_model_path ./qwen2.5-7b-functioncall/checkpoint-500 \\\n",
    "    --output_dir ./output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c9a8bd-e7e8-4b59-b7d3-ca2e9e6498f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: '==2.5.8'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flash_attn ==2.5.8 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47877e8-888a-4019-919e-d60ed9c3964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566d2e11314d493e8abb094a636da77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./output_dir')\n",
    "model = AutoModelForCausalLM.from_pretrained('./output_dir')\n",
    "model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02e5c2e4-9c03-4e5a-bdc5-1db31c594d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":system_prompt +'ì˜¤ëŠ˜ ë‚ ì§œëŠ” 2025-03-09 ì…ë‹ˆë‹¤.'},\n",
    "    {\"role\": \"user\", \"content\": \"í…ŒìŠ¬ë¼ì˜ ìµœê·¼ ì„±ì¥ë¥ ì€ ì–´ë•Œ?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac9e48a-b063-4917-9356-1ca1ac3413b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.decode(model_inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7403a237-7db8-4337-b5c3-e7e9f9457c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” AI ëª¨ë¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ íŠ¹ì • í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê³ , í•¨ìˆ˜ì˜ ì‹œê·¸ë‹ˆì²˜ëŠ” <tools></tools> XML íƒœê·¸ ë‚´ì— ì œê³µë©ë‹ˆë‹¤. í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•ŒëŠ” í•¨ìˆ˜ì— í•„ìš”í•œ ì •í™•í•œ ê°’ì„ ì¶”ì •í•˜ì§€ ë§ê³  ì‚¬ìš©ìê°€ ì œê³µí•œ ê°’ì— ë”°ë¼ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì•„ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤ê³¼ ê°ê°ì˜ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤:\n",
      "\n",
      "get_issues_summarized\n",
      "\n",
      "ì„¤ëª…: íŠ¹ì • íšŒì‚¬ ë˜ëŠ” í‚¤ì›Œë“œì— ëŒ€í•œ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.\n",
      "íŒŒë¼ë¯¸í„°:\n",
      "keyword: ì´ìŠˆ/í˜„í™©ì„ ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” íšŒì‚¬ëª… ë˜ëŠ” í‚¤ì›Œë“œ.\n",
      "days: ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” ê¸°ê°„(ì¼ ë‹¨ìœ„ (integer)).\n",
      "\n",
      "get_reddit_hotissue\n",
      "\n",
      "ì„¤ëª…: ê¸ˆìœµì‹œì¥ì—ì„œ í•«í•œ ì´ìŠˆë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.\n",
      "íŒŒë¼ë¯¸í„°:\n",
      "days: ê²€ìƒ‰í•˜ê³ ì í•˜ëŠ” ê¸°ê°„(ì¼ ë‹¨ìœ„).\n",
      "\n",
      "{name: get_earnings,\n",
      "ì„¤ëª…: ê¸°ì—…ì˜ ì¬ë¬´ì¬í‘œ ë˜ëŠ” í˜„ê¸ˆíë¦„ì„ ê°€ì ¸ì˜¤ê³  ë¶„ì„í•©ë‹ˆë‹¤. ì„±ì¥ë¥ ê³¼ ê°™ì´ ì´ì „ ë…„ë„ ë°ì´í„°ê°€ í•„ìš”í•œê²½ìš°, ì´ì „ ë…„ë„ ë°ì´í„°ë„ í•œë²ˆ ë” í˜¸ì¶œí•˜ì„¸ìš”\n",
      "íŒŒë¼ë¯¸í„°:{symbol: ì‹¤ì  ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ê¸°ì—…ì˜ ì‹¬ë³¼.\n",
      "analysis_type: ë¶„ì„ ìœ í˜•(growth, profitability, stability, valuation, cashflow, dividend, cost, NA).\n",
      "type_: ë°ì´í„° íƒ€ì…(yearly ë˜ëŠ” quarter).\n",
      "year: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ì—°ë„ (ëª…ì‹œí•˜ì§€ ì•Šì„ê²½ìš°, ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ìµœê·¼ ì—°ë„ì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.).\n",
      "quarter: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ë¶„ê¸°(ëª…ì‹œí•˜ì§€ì•Šì€ ê²½ìš° ìµœê·¼ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ë„ë¡ -1 ì„ ì…ë ¥í•©ë‹ˆë‹¤).}}\n",
      "\n",
      "get_consensus\n",
      "\n",
      "ì„¤ëª…: ê¸°ì—…ì˜ EPS ì»¨ì„¼ì„œìŠ¤ ë°ì´í„° ë˜ëŠ” ë§¤ìˆ˜/ë§¤ë„/í™€ë“œ ì˜ê²¬ì„ ê°€ì ¸ì™€ì„œ ë¶„ì„í•©ë‹ˆë‹¤.\n",
      "íŒŒë¼ë¯¸í„°:\n",
      "symbol: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ê¸°ì—…ì˜ ì‹¬ë³¼.\n",
      "year: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ì—°ë„.\n",
      "quarter: ë°ì´í„°ë¥¼ ì°¾ê³ ì í•˜ëŠ” ë¶„ê¸°.\n",
      "ê° í•¨ìˆ˜ í˜¸ì¶œ ì‹œ, JSON ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ ì´ë¦„ê³¼ ì¸ìë“¤ì„ <tool_call></tool_call> XML íƒœê·¸ ë‚´ì— ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. í•¨ìˆ˜ í˜¸ì¶œ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "xml\n",
      "Copy\n",
      "<tool_call>\n",
      "{\n",
      "    \"name\": \"get_earnings\",\n",
      "    \"arguments\": {\n",
      "        \"symbol\": \"AAPL\",\n",
      "        \"analysis_type\": \"growth\",\n",
      "        \"type_\": \"yearly\" ,\n",
      "        \"year\": \"2024\",\n",
      "        \"quarter\": \"-1\"\n",
      "    }\n",
      "}\n",
      "</tool_call>\n",
      "ê° í•¨ìˆ˜ì˜ ì¸ì ê°’ì„ ì •í™•í•˜ê²Œ ì§€ì •í•´ ì£¼ì„¸ìš”. íŠ¹íˆ, ì—°ë„ì™€ ë¶„ê¸°ë¥¼ ì„¤ì •í•  ë•Œ í˜„ì¬ ë‚ ì§œê°€ 1ì›”ì´ë‚˜ 2ì›”ì¸ ê²½ìš°, ìµœê·¼ ì—°ë„ì˜ ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "**ì£¼ì˜ì‚¬í•­\n",
      "í•¨ìˆ˜ë¥¼ í˜¸ì¶œí• ë•Œë¥¼ ì œì™¸í•˜ê³  í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.\n",
      "\n",
      "ì˜¤ëŠ˜ ë‚ ì§œëŠ” 2025-03-09 ì…ë‹ˆë‹¤.<|im_end|>\n",
      "<|im_start|>user\n",
      "í…ŒìŠ¬ë¼ì˜ ìµœê·¼ ì„±ì¥ë¥ ì€ ì–´ë•Œ?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<tool_call>{\"name\":\"get_earnings\",\"arguments\": {\"symbol\": \"TSLA\", \"analysis_type\": \"growth\", \"type_\": \"yearly\", \"year\": \"2024\", \"quarter\": \"-1\"}}</tool_call><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "eos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.module.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=2048, eos_token_id=eos_token_id)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a0a125-cb6f-4eea-ae00-4e7e6aa1ca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: â€”token 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\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login â€”token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7eb6a32-509e-429e-9b94-b2c4f5cefe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "username = \"irene93\"\n",
    "\n",
    "MODEL_NAME = 'functioncall_stkissue'\n",
    "token ='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aab35e9c-079b-4dea-98da-81b8622270ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4867de9db1de403db85bf57952e806f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2cecf30cd149da8616da13ffc2ca7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb9b29104da4a26bcfde517ab671069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f57aae4977d40c1bae705939e129a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45cc31f850442fd966152de3abf302f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f2932cfff149308ccaced7f3caa99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/irene93/functioncall_stkissue/commit/cd9c86a33b90beb525ac097aa1a077087db69851', commit_message='Upload folder using huggingface_hub', commit_description='', oid='cd9c86a33b90beb525ac097aa1a077087db69851', pr_url=None, repo_url=RepoUrl('https://huggingface.co/irene93/functioncall_stkissue', endpoint='https://huggingface.co', repo_type='model', repo_id='irene93/functioncall_stkissue'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(\n",
    "    token=token, ### í† í°ê°’ ,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    token=token,### í† í°ê°’ ,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path=\"output_dir\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc5823-35d1-48c9-acdc-bde2e302e642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
